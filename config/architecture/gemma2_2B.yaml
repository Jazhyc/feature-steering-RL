# Architecture Configuration
model:
  name: "gemma-2-2b-it" # Name on transformer_lens
  device: "cuda"
  dtype: "bfloat16"

sae:
  release: "gemma-scope-2b-pt-res"
  sae_id: "layer_12/width_65k/average_l0_21"
  use_lora_adapter: true
  lora_rank: 64
  lora_alpha: ${.lora_rank}
  fusion_mode: "additive"

dataset:
  name: "princeton-nlp/llama3-ultrafeedback-armorm"
  train_split: "train"
  eval_split: "test"
  sample_size: null  # Set to a number to limit dataset size for testing

backup_chat_template: "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"