# Architecture Configuration
model:
  name: "gpt2"
  device: "cuda"  # Will be auto-detected if cuda is available
  dtype: "bfloat16"

sae:
  release: "gpt2-small-res-jb"
  sae_id: "blocks.7.hook_resid_pre"
  use_lora_adapter: true
  lora_rank: 64
  lora_alpha: 32
  fusion_mode: "additive"

dataset:
  name: "princeton-nlp/llama3-ultrafeedback-armorm"
  split: "train"
  sample_size: 1  # Set to a number to limit dataset size for testing

backup_chat_template: "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"
