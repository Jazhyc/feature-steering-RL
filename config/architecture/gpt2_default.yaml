# Architecture Configuration
model:
  name: "gpt2" # Name on transformer_lens
  device: "cuda"
  dtype: "bfloat16"

sae:
  release: "gpt2-small-res-jb"
  sae_id: "blocks.7.hook_resid_pre"
  use_lora_adapter: false
  lora_rank: 64
  lora_alpha: ${.lora_rank}
  fusion_mode: "additive"
  dtype: "bfloat16"

dataset:
  name: "princeton-nlp/llama3-ultrafeedback-armorm"
  train_split: "train"
  eval_split: "test"
  sample_size: 8000  # Set to a number to limit dataset size for testing, null for full dataset
  dataset_num_proc: 8
  chat_template: "{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'system' %}\n{{ '<|system|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}"
