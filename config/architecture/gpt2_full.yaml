# Architecture Configuration for full GPT2 training (no SAE)
use_sae: false

model:
  name: "gpt2" # Name on transformer_lens
  device: "cuda"
  dtype: "bfloat16"

# SAE config still present but not used when use_sae is false
sae:
  release: "gpt2-small-res-jb"
  sae_id: "blocks.7.hook_resid_pre"
  activation_type: "soft_threshold"  # Options: "soft_threshold", "jump_relu", "relu"
  initial_threshold: 0.01
  bandwidth: 0.05
  dtype: "bfloat16"

dataset:
  name: "princeton-nlp/llama3-ultrafeedback-armorm"
  train_split: "train"
  eval_split: "test"
  sample_size: 1000  # Set to a number to limit dataset size for testing, null for full dataset
  dataset_num_proc: 8
  chat_template: "{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'system' %}\n{{ '<|system|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}"
