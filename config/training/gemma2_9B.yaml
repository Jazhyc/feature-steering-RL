# Training Configuration for Gemma2-9B with SAE (same hyperparameters as 2B)

# Training Configuration
output_dir: "logs/training_run"
num_train_epochs: 10
learning_rate: 5e-6

# Batch size configuration
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
dataloader_num_workers: 20
dataloader_pin_memory: true
dataset_num_proc: 20

# Training optimizations
# optim: "adamw_8bit"
bf16: true
torch_compile: true
lr_scheduler_type: cosine
warmup_ratio: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
gradient_accumulation_steps: 16
activation_offloading: false
deepspeed: config/deepspeed.json

# SimPO specific variables
beta: 10
gamma_beta_ratio: 0.5
max_prompt_length: 1400
max_length: 1600
l0_act_coeff: 0
l1_act_coeff: 0.02
l2_act_coeff: 0
max_grad_norm: 1.0
log_threshold_lr_multiplier: 1e3  # Only used when use_jump_relu is true
penalty_warmup_ratio: 0.0

# Additional training parameters
save_steps: 100000
logging_steps: 10
save_total_limit: 10
eval_strategy: "steps"
eval_epoch_fraction: 0.2 # non trainer argument needs to be popped
load_best_model_at_end: false
remove_unused_columns: false
