# Training Configuration
output_dir: "logs/training_run"
num_train_epochs: 1
learning_rate: 1e-4

# Batch size configuration
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
dataloader_num_workers: 8
dataloader_pin_memory: true

# Training optimizations
bf16: true
optim: "adamw_torch_fused"
torch_compile: true
lr_scheduler_type: cosine
warmup_ratio: 0.1

# Additional training parameters
save_steps: 10000
logging_steps: 10
max_prompt_length: 128 # Should be 1800 for gemma (based on dataset)
max_length: 512 # Should be 2048 for gemma
save_total_limit: 2
eval_strategy: "steps"
eval_epoch_fraction: 0.2 # non trainer argument needs to be popped
load_best_model_at_end: false
remove_unused_columns: false
