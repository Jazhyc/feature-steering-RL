# Minimal config for steering fraction evaluation experiment

wandb:
  project: fsrl-steering-fraction-eval
  entity: feature-steering-RL
  dir: "logs"
  mode: online
  tags: [eval, steering-fraction]
  notes: "Validation loss vs. steering fraction"

# For optional downloads via wandb_utils
wandb_entity: feature-steering-RL
wandb_project_family: Gemma2-2B-clean
# When auto_download=true, set the specific completed run name to fetch
wandb_run_name: mild-glade-10
auto_download: true
force_download: false

interval_percent: 10  # 10 -> evaluate at 10%, 20%, ..., 100%

architecture:
  use_sae: true
  model:
    name: gemma-2-2b-it
    device: cuda
    dtype: bfloat16
  # Required for this experiment: local path to a trained adapter folder containing adapter_weights.safetensors
  adapter_local_path: null
  dataset:
    name: princeton-nlp/llama3-ultrafeedback-armorm
    train_split: train
    eval_split: test
    # same chat template as in architecture/gemma2_2B.yaml
    chat_template: "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] | trim + '\n\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + content | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"
    sample_size: null
    dataset_num_proc: 20

training:
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 1
  eval_steps: 100
  # SimPO parameters to match gemma2_2B.yaml
  beta: 10
  gamma_beta_ratio: 0.5
