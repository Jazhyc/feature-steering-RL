# Minimal config for feature ablation evaluation experiment

wandb:
  project: fsrl-feature-ablation-eval
  entity: feature-steering-RL
  dir: "logs"
  mode: online
  tags: [eval, feature-ablation, alignment, style]
  notes: "Validation loss when turning off alignment vs style features"

# For optional downloads via wandb_utils
wandb_entity: feature-steering-RL
wandb_project_family: Gemma2-2B-muon
# When auto_download=true, set the specific completed run name to fetch
wandb_run_name: mild-resonance-1
auto_download: true
force_download: false

# Classification files containing DeepSeek labels
alignment_classification_file: "${oc.env:PWD}/outputs/feature_classification/gemma-2-2b/12-gemmascope-res-65k_canonical_alignment_classified_deepseek-deepseek-chat-v3-0324.json"
style_classification_file: "${oc.env:PWD}/outputs/feature_classification/gemma-2-2b/12-gemmascope-res-65k_canonical_formatting_classified_deepseek-deepseek-chat-v3-0324.json"

architecture:
  use_sae: true
  model:
    name: gemma-2-2b-it # why is this needed?
    device: cuda
    dtype: bfloat16
  # Required for this experiment: local path to a trained adapter folder containing adapter_weights.safetensors
  adapter_local_path: null
  dataset:
    name: princeton-nlp/llama3-ultrafeedback-armorm
    train_split: train
    eval_split: test
    # same chat template as in architecture/gemma2_2B.yaml
    chat_template: "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] | trim + '\n\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + content | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"
    sample_size: null
    dataset_num_proc: 20

training:
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 1
  eval_steps: 100
  # SimPO parameters to match gemma2_2B.yaml
  beta: 10
  gamma_beta_ratio: 0.5

hydra:
  run:
    dir: logs/hydra_runs/${now:%Y-%m-%d_%H-%M-%S}