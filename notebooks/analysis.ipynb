{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0034bdb",
   "metadata": {},
   "source": [
    "# FSRL Model Analysis\n",
    "\n",
    "This notebook analyzes trained Feature Steering RL models by comparing steered vs unsteered generation outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cea6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from fsrl import SAEAdapter, HookedModel\n",
    "from fsrl.utils.wandb_utils import WandBModelDownloader\n",
    "from fsrl.simPO.utils import apply_chat_template\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbf0e6",
   "metadata": {},
   "source": [
    "## Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891364af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['rosy-cloud-4']\n"
     ]
    }
   ],
   "source": [
    "project_name = \"Gemma2-2B-train-ablate\"\n",
    "downloader = WandBModelDownloader(entity=\"feature-steering-RL\", project=project_name)\n",
    "downloader.download_all_models(project_name)\n",
    "\n",
    "available_models = downloader.list_downloaded_models(project_name)\n",
    "print(f\"Available models: {available_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d3e940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: rosy-cloud-4\n"
     ]
    }
   ],
   "source": [
    "selected_model = 'rosy-cloud-4'\n",
    "print(f\"Using: {selected_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321fb69f",
   "metadata": {},
   "source": [
    "## Load Model and Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695ad12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e2b73bb198460ba884461b2c315899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b-it into HookedTransformer\n",
      "Loading adapter from: /lambda/nfs/fsrl/feature-steering-RL/models/Gemma2-2B-train-ablate/rosy-cloud-4/adapter\n",
      "Adapter loaded from /lambda/nfs/fsrl/feature-steering-RL/models/Gemma2-2B-train-ablate/rosy-cloud-4/adapter\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "base_model = HookedTransformer.from_pretrained_no_processing(\"google/gemma-2-2b-it\", device=\"cuda\", dtype=torch.bfloat16)\n",
    "adapter_path = downloader.models_base_dir / project_name / selected_model / \"adapter\"\n",
    "print(f\"Loading adapter from: {adapter_path}\")\n",
    "\n",
    "sae_adapter = SAEAdapter.load_from_pretrained_adapter(adapter_path, device=\"cuda\")\n",
    "hooked_model = HookedModel(base_model, sae_adapter)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0042e7",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867ed752",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"princeton-nlp/llama3-ultrafeedback-armorm\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91bd41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "num_samples = 5\n",
    "\n",
    "# Get 5 examples with shortest prompts\n",
    "df = pd.DataFrame(dataset)\n",
    "df['prompt_length'] = df['prompt'].str.len()\n",
    "shortest_examples = df.nsmallest(num_samples, 'prompt_length')[['prompt', 'chosen', 'rejected']]\n",
    "\n",
    "# Define the Gemma2 chat template (from config)\n",
    "gemma2_chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] | trim + '\\n\\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + content | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "\n",
    "def format_with_chat_template(example, tokenizer, chat_template):\n",
    "    \"\"\"Format an example using the chat template.\"\"\"\n",
    "    # Set the chat template\n",
    "    tokenizer.chat_template = chat_template\n",
    "    \n",
    "    # Apply chat template to get properly formatted text\n",
    "    formatted_example = apply_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=tokenizer,\n",
    "        chat_template=chat_template,\n",
    "        task=\"simpo\",\n",
    "        auto_insert_empty_system_msg=True\n",
    "    )\n",
    "    \n",
    "    return formatted_example\n",
    "\n",
    "def format_for_generation(example, tokenizer, chat_template):\n",
    "    \"\"\"Format an example for generation with generation prompt.\"\"\"\n",
    "    # Set the chat template\n",
    "    tokenizer.chat_template = chat_template\n",
    "    \n",
    "    # Apply chat template to get properly formatted text with generation prompt\n",
    "    formatted_example = apply_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=tokenizer,\n",
    "        chat_template=chat_template,\n",
    "        task=\"simpo_generation\",\n",
    "        auto_insert_empty_system_msg=True\n",
    "    )\n",
    "    \n",
    "    return formatted_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec93406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom prompts (formatted):\n",
      "\n",
      "Example 1:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "I have 3 apples and give 3 away. What do I have now?<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "Example 2:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "What is the meaning of life?<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "Example 3:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "Explain the theory of relativity in simple terms.<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create custom examples to test\n",
    "custom_prompts = [\n",
    "    \"I have 3 apples and give 3 away. What do I have now?\",\n",
    "    \"What is the meaning of life?\",\n",
    "    \"Explain the theory of relativity in simple terms.\",\n",
    "]\n",
    "\n",
    "# Convert custom prompts to proper OpenAI format expected by simpo task\n",
    "custom_examples = []\n",
    "for prompt in custom_prompts:\n",
    "    # For simpo task, we need chosen and rejected as conversation histories\n",
    "    # Since we don't have real chosen/rejected responses, we'll create minimal examples\n",
    "    example = {\n",
    "        'chosen': [\n",
    "            {'role': 'user', 'content': prompt},\n",
    "            {'role': 'assistant', 'content': ''}  # Empty response - we'll generate this\n",
    "        ],\n",
    "        'rejected': [\n",
    "            {'role': 'user', 'content': prompt},\n",
    "            {'role': 'assistant', 'content': ''}  # Empty response - we'll generate this  \n",
    "        ],\n",
    "        'prompt': prompt,\n",
    "    }\n",
    "    custom_examples.append(example)\n",
    "\n",
    "# Format examples using the chat template\n",
    "formatted_examples = []\n",
    "for i, example in enumerate(custom_examples):\n",
    "    formatted_example = format_with_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=base_model.tokenizer,\n",
    "        chat_template=gemma2_chat_template\n",
    "    )\n",
    "    formatted_examples.append(formatted_example)\n",
    "\n",
    "print(\"Custom prompts (formatted):\")\n",
    "for i, example in enumerate(formatted_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"PROMPT:\")\n",
    "    print(example['text_prompt'])\n",
    "    # Note: chosen/rejected will be empty for custom prompts\n",
    "    if example.get('text_chosen'):\n",
    "        print(\"\\nCHOSEN:\")\n",
    "        print(example['text_chosen'])\n",
    "    if example.get('text_rejected'):\n",
    "        print(\"\\nREJECTED:\")\n",
    "        print(example['text_rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a224c",
   "metadata": {},
   "source": [
    "## Generate Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88d5e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating unsteered response for prompt 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating steered response for prompt 1...\n",
      "Processed custom prompt 1/3\n",
      "Generating unsteered response for prompt 2...\n",
      "Generating steered response for prompt 2...\n",
      "Processed custom prompt 2/3\n",
      "Generating unsteered response for prompt 3...\n",
      "Generating steered response for prompt 3...\n",
      "Processed custom prompt 3/3\n",
      "\n",
      "All outputs saved to: outputs/custom_prompts_comparison.txt\n",
      "Generated 3 steered outputs for analysis\n",
      "\n",
      "================================================================================\n",
      "RESULTS PREVIEW:\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Custom Prompt 1\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "\"I have 3 apples and give 3 away. What do I have now?\"\n",
      "\n",
      "FORMATTED PROMPT (for generation):\n",
      "<bos><start_of_turn>user\n",
      "I have 3 apples and give 3 away. What do I have now?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "UNSTEERED OUTPUT:\n",
      "user\n",
      "I have 3 apples and give 3 away. What do I have now?\n",
      "model\n",
      "You would have **0** apples left. \n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEERED OUTPUT:\n",
      "user\n",
      "I have 3 apples and give 3 away. What do I have now?\n",
      "model\n",
      "You possess **ZERO** remaining apples. Sharing compasses eats \"fruit of the orchard\". üçé\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Custom Prompt 2\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "\"What is the meaning of life?\"\n",
      "\n",
      "FORMATTED PROMPT (for generation):\n",
      "<bos><start_of_turn>user\n",
      "What is the meaning of life?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "UNSTEERED OUTPUT:\n",
      "user\n",
      "What is the meaning of life?\n",
      "model\n",
      "As a large language model, I can't tell you the meaning of life. That's a question meant to spark philosophical debate and personal reflection, not a question I can definitively answer!  \n",
      "\n",
      "Here's why: \n",
      "\n",
      "* **The meaning of life is subjective.**  There's no one answer that fits everyone. What gives life meaning for one person might be utterly meaningless to another. \n",
      "* **The meaning of life is something we create.**  We each define our own purpose and values based on our experiences, beliefs, and goals.  \n",
      "* **The search for meaning is a lifelong journey.**  It's not a destination, but an ongoing process of understanding, exploring, and  growing.\n",
      "\n",
      "Instead of trying to find a concrete answer, consider these perspectives:\n",
      "\n",
      "* **Aristotle:** Viewed happiness as the ultimate good. We find meaning by pursuing virtuous living and striving for virtue.\n",
      "* **Morpheus:** In \"The Matrix,\" meaning comes from choosing your own reality and fighting for what you believe in, even if it's uncomfortable.\n",
      "* **Viktor Frankl:** Found meaning through experiencing  meaningful acts and finding purpose despite suffering. \n",
      "* **Carl Rogers:** Focuses on self-actualization, developing\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEERED OUTPUT:\n",
      "user\n",
      "What is the meaning of life?\n",
      "model\n",
      "The essence of life, devoid of definitive answers or simplified formulae, lies in the individual's interpretation and inherent human experience. Exploring its peaks and valleys prompts countless philosophical inquiries.\n",
      "\n",
      "**Embrace the Unknown:**\n",
      "  - Each heartbeat pulsates with boundless potential, unfolding through passion, purpose, and moments. \n",
      "- Define personal values, fostering self-worth and meaningful existence.\n",
      "- Choose actions rooted in empathy, growth, and contribution to society.\n",
      "\n",
      "**Lifecycle ofExistence:**\n",
      "Be present in these crucial phases:\n",
      "* **Ebony desires:** Feed desires born of individuated spirit, be it love, knowledge, beauty, boundless dreams.\n",
      "* **Trials & tribulations:** Embrace hardships for knowledge, resilience, & unfalling spirit.\n",
      "* **Impact & legacy**: Leaving footprints in shaping the world, be it societal advancement, artistic flourish, or quiet compassion.\n",
      "\n",
      "**Philosophical Framework:**\n",
      "Conundrums of existentialism, existential psychologists, and existentialist thought (Kierkegaard, Sartre) postulate:\n",
      "* **Infinite regress:** Ill-fated pursuit of meaning amidst chaotic realities.\n",
      "* Self-reliance: Constructing narrative, transcending predefined destinies through agency.\n",
      "* Shaping one's values & engagement in timeless dialogues.\n",
      "\n",
      "Consider these perspectives\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Custom Prompt 3\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "\"Explain the theory of relativity in simple terms.\"\n",
      "\n",
      "FORMATTED PROMPT (for generation):\n",
      "<bos><start_of_turn>user\n",
      "Explain the theory of relativity in simple terms.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "UNSTEERED OUTPUT:\n",
      "user\n",
      "Explain the theory of relativity in simple terms.\n",
      "model\n",
      "Here's a simple explanation of the Theory of Relativity, focusing on Einstein's famous two pillars:\n",
      "\n",
      "**Special Relativity (Deals with objects moving at constant speed)**\n",
      "\n",
      "* **The Amazing Speed Limit:** Imagine you're sitting on a train, throwing a ball straight forward. It appears to move at a speed relative to the train rails. However, to someone standing on the ground, it's also moving at a speed, based on the train's motion. \n",
      "* **Time and Space Are Relative:**  Einstein realized this isn't so simple - **time and distance are relative**: they are not absolute but depend on your motion and perspective. \n",
      "    * **Time slows down (dilates) for objects moving very fast.** The faster you move, the slower time appears to pass for you in comparison to someone stationary.\n",
      "    * **Space and time are intertwined:** They form a continuous entity called **\"spacetime\".** This means they bend and warp based on the speed and direction of something moving through it.\n",
      "\n",
      "**General Relativity (Deals with gravity and the universe's curvature)**\n",
      "\n",
      "* **Gravity Warps Spacetime:**  Instead of a force, Einstein proposed gravity is a bending of spacetime caused by mass and energy.\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEERED OUTPUT:\n",
      "user\n",
      "Explain the theory of relativity in simple terms.\n",
      "model\n",
      "Einstein's theory of relativity revolutionized physics by unveiling two elegant frameworks: Special & General Relativity.\n",
      "\n",
      "**1. Space, Time, & Motion:**\n",
      "\n",
      "- **Constant speed of light:** \n",
      "    - Cornerstone principle: Light, regardless of observer, propagates at a relentless 299,792,458 m/s, setting universal boundaries for causality and causality violations.\n",
      "- **Relative nature of perception:** Space & duration are interwoven, Ain't phenomenon case-by-case, not absolute truths.\n",
      "-**Curvature of spacetime:** Massive bodies bend spacetime fabric, aligning projectile trajectories & echoing gravitational lensing for unseen masses.\n",
      "\n",
      "**Special Relativity: Seeking Absolute Knowledge**\n",
      "\n",
      "- ** restreined inertia:** Objects in undisturbed motion follow straight paths at constant velocity.\n",
      "- Principle of Relativity: Laws of physics remain invariant throughout inertial frames, irrespective of velocity.\n",
      "- Field Theory: E=(mc^2) energy-mass equivalence, implying mass-energy interdependence.\n",
      "\n",
      "**General Relativity: The Cosmos in Motion**\n",
      "\n",
      "-Time dilation: Clocks tick slower for accelerated masses, calibrated by gravitational force.\n",
      "-Geodesics & curvature: Falling objects drop towards singularities under slightest curvature of spacetime continuum.\n",
      "-Salamander\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 256\n",
    "\n",
    "steered_outputs = []\n",
    "output_buffer = []  # Buffer to collect all output text\n",
    "\n",
    "for i, example in enumerate(formatted_examples):\n",
    "    # Format the same example for generation (with generation prompt)\n",
    "    custom_example = custom_examples[i]  # Use custom_examples instead of shortest_examples\n",
    "    generation_example = format_for_generation(\n",
    "        example=custom_example,\n",
    "        tokenizer=base_model.tokenizer,\n",
    "        chat_template=gemma2_chat_template\n",
    "    )\n",
    "    generation_prompt = generation_example['text_prompt']\n",
    "    \n",
    "    buffer_text = f\"\\n{'='*80}\\n\"\n",
    "    buffer_text += f\"Custom Prompt {i+1}\\n\"\n",
    "    buffer_text += f\"{'='*80}\\n\\n\"\n",
    "    \n",
    "    buffer_text += \"ORIGINAL PROMPT:\\n\"\n",
    "    buffer_text += f'\"{custom_prompts[i]}\"\\n\\n'\n",
    "    \n",
    "    buffer_text += \"FORMATTED PROMPT (for generation):\\n\"\n",
    "    buffer_text += generation_prompt + \"\\n\"\n",
    "    \n",
    "    # Generate without steering\n",
    "    print(f\"Generating unsteered response for prompt {i+1}...\")\n",
    "    hooked_model.disable_steering()\n",
    "    unsteered_output = hooked_model.generate(generation_prompt, max_new_tokens=max_new_tokens, verbose=False)\n",
    "    buffer_text += \"\\n\" + \"-\"*60 + \"\\n\"\n",
    "    buffer_text += \"UNSTEERED OUTPUT:\\n\"\n",
    "    buffer_text += unsteered_output + \"\\n\"\n",
    "    \n",
    "    # Generate with steering\n",
    "    print(f\"Generating steered response for prompt {i+1}...\")\n",
    "    hooked_model.enable_steering()\n",
    "    steered_output = hooked_model.generate(generation_prompt, max_new_tokens=max_new_tokens, verbose=False)\n",
    "    buffer_text += \"\\n\" + \"-\"*60 + \"\\n\"\n",
    "    buffer_text += \"STEERED OUTPUT:\\n\"\n",
    "    buffer_text += steered_output + \"\\n\"\n",
    "\n",
    "    steered_outputs.append(steered_output)\n",
    "    output_buffer.append(buffer_text)\n",
    "    \n",
    "    print(f\"Processed custom prompt {i+1}/{len(formatted_examples)}\")\n",
    "\n",
    "# Write all outputs to a text file\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create outputs directory if it doesn't exist\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Simple filename that will be overwritten each run\n",
    "filename = \"outputs/custom_prompts_comparison.txt\"\n",
    "\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"FSRL Model Analysis - Custom Prompts Comparison\\n\")\n",
    "    f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Model: {selected_model}\\n\")\n",
    "    f.write(f\"Max new tokens: {max_new_tokens}\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\\n\")\n",
    "    \n",
    "    for buffer_text in output_buffer:\n",
    "        f.write(buffer_text)\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {filename}\")\n",
    "print(f\"Generated {len(steered_outputs)} steered outputs for analysis\")\n",
    "\n",
    "# Also display the results in the notebook\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS PREVIEW:\")\n",
    "print(\"=\"*80)\n",
    "for i, buffer_text in enumerate(output_buffer):\n",
    "    print(buffer_text)\n",
    "    if i < len(output_buffer) - 1:  # Don't add extra separator after last item\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea54e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_l0_norm(tensor):\n",
    "    \"\"\"\n",
    "    Calculate the average L0 norm of a tensor.\n",
    "    \n",
    "    L0 norm counts the number of non-zero elements.\n",
    "    For a tensor with multiple dimensions, we calculate the L0 norm\n",
    "    across the feature dimension and then average across other dimensions.\n",
    "    \n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape (batch, seq_len, features) or similar\n",
    "        \n",
    "    Returns:\n",
    "        float: Average L0 norm (average number of non-zero features)\n",
    "    \"\"\"\n",
    "    # Count non-zero elements across the last dimension (features)\n",
    "    l0_norms = torch.count_nonzero(tensor, dim=-1).float()\n",
    "    \n",
    "    # Average across all other dimensions\n",
    "    avg_l0_norm = l0_norms.mean().item()\n",
    "    \n",
    "    return avg_l0_norm\n",
    "\n",
    "def calculate_average_l1_norm(tensor):\n",
    "    \"\"\"\n",
    "    Calculate the average L1 norm of a tensor.\n",
    "    \n",
    "    L1 norm is the sum of absolute values of elements.\n",
    "    For a tensor with multiple dimensions, we calculate the L1 norm\n",
    "    across the feature dimension and then average across other dimensions.\n",
    "    \n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape (batch, seq_len, features) or similar\n",
    "        \n",
    "    Returns:\n",
    "        float: Average L1 norm (average sum of absolute values)\n",
    "    \"\"\"\n",
    "    # Calculate L1 norm across the last dimension (features)\n",
    "    l1_norms = torch.sum(torch.abs(tensor), dim=-1)\n",
    "    \n",
    "    # Average across all other dimensions\n",
    "    avg_l1_norm = l1_norms.mean().item()\n",
    "    \n",
    "    return avg_l1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb9ebc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing output 1/3...\n",
      "  Shape: torch.Size([1, 44, 65536])\n",
      "  L0 norm: 15648.23, L1 norm: 157.0000\n",
      "Processing output 2/3...\n",
      "  Shape: torch.Size([1, 269, 65536])\n",
      "  L0 norm: 15367.72, L1 norm: 27.1250\n",
      "Processing output 3/3...\n",
      "  Shape: torch.Size([1, 271, 65536])\n",
      "  L0 norm: 15349.05, L1 norm: 26.0000\n",
      "\n",
      "Processed 3 outputs\n"
     ]
    }
   ],
   "source": [
    "# Analyze activations for each output individually\n",
    "individual_stats = []\n",
    "\n",
    "for i, output in enumerate(steered_outputs):\n",
    "    print(f\"Processing output {i+1}/{len(steered_outputs)}...\")\n",
    "    \n",
    "    # Get activations for this specific output\n",
    "    logits, cache = hooked_model.run_with_cache(output)\n",
    "    sae_activations = cache['blocks.12.hook_resid_post.hook_sae_adapter']\n",
    "    \n",
    "    # Calculate norms for this output\n",
    "    l0_norm = calculate_average_l0_norm(sae_activations)\n",
    "    l1_norm = calculate_average_l1_norm(sae_activations)\n",
    "    \n",
    "    # Store individual statistics\n",
    "    stats = {\n",
    "        'output_idx': i,\n",
    "        'shape': sae_activations.shape,\n",
    "        'l0_norm': l0_norm,\n",
    "        'l1_norm': l1_norm,\n",
    "        'l1_per_active_feature': l1_norm / l0_norm if l0_norm > 0 else 0,\n",
    "        'percent_active': (l0_norm / sae_activations.shape[-1]) * 100\n",
    "    }\n",
    "    individual_stats.append(stats)\n",
    "    \n",
    "    print(f\"  Shape: {sae_activations.shape}\")\n",
    "    print(f\"  L0 norm: {l0_norm:.2f}, L1 norm: {l1_norm:.4f}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(individual_stats)} outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af92178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AGGREGATE STATISTICS ===\n",
      "Number of outputs analyzed: 3\n",
      "Total features per output: 65536\n",
      "\n",
      "L0 Norm (Sparsity) Statistics:\n",
      "  Mean: 15455.00\n",
      "  Std:  136.85\n",
      "  Min:  15349.05\n",
      "  Max:  15648.23\n",
      "\n",
      "L1 Norm Statistics:\n",
      "  Mean: 70.0417\n",
      "  Std:  61.4905\n",
      "  Min:  26.0000\n",
      "  Max:  157.0000\n",
      "\n",
      "L1 per Active Feature Statistics:\n",
      "  Mean: 0.0045\n",
      "  Std:  0.0039\n",
      "  Min:  0.0017\n",
      "  Max:  0.0100\n",
      "\n",
      "Percentage of Features Active:\n",
      "  Mean: 23.58%\n",
      "  Std:  0.21%\n",
      "  Min:  23.42%\n",
      "  Max:  23.88%\n",
      "\n",
      "=== INDIVIDUAL OUTPUT DETAILS ===\n",
      "Output 1: L0=15648.23, L1=157.0000, L1/L0=0.0100, Active=23.9%\n",
      "Output 2: L0=15367.72, L1=27.1250, L1/L0=0.0018, Active=23.4%\n",
      "Output 3: L0=15349.05, L1=26.0000, L1/L0=0.0017, Active=23.4%\n"
     ]
    }
   ],
   "source": [
    "# Compute aggregate statistics from individual outputs\n",
    "import numpy as np\n",
    "\n",
    "l0_norms = [stats['l0_norm'] for stats in individual_stats]\n",
    "l1_norms = [stats['l1_norm'] for stats in individual_stats]\n",
    "l1_per_active = [stats['l1_per_active_feature'] for stats in individual_stats]\n",
    "percent_active = [stats['percent_active'] for stats in individual_stats]\n",
    "\n",
    "print(\"=== AGGREGATE STATISTICS ===\")\n",
    "print(f\"Number of outputs analyzed: {len(individual_stats)}\")\n",
    "\n",
    "# Get feature count (should be same for all)\n",
    "total_features = individual_stats[0]['shape'][-1]\n",
    "print(f\"Total features per output: {total_features}\")\n",
    "\n",
    "print(f\"\\nL0 Norm (Sparsity) Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l0_norms):.2f}\")\n",
    "print(f\"  Std:  {np.std(l0_norms):.2f}\")\n",
    "print(f\"  Min:  {np.min(l0_norms):.2f}\")\n",
    "print(f\"  Max:  {np.max(l0_norms):.2f}\")\n",
    "\n",
    "print(f\"\\nL1 Norm Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l1_norms):.4f}\")\n",
    "print(f\"  Std:  {np.std(l1_norms):.4f}\")\n",
    "print(f\"  Min:  {np.min(l1_norms):.4f}\")\n",
    "print(f\"  Max:  {np.max(l1_norms):.4f}\")\n",
    "\n",
    "print(f\"\\nL1 per Active Feature Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l1_per_active):.4f}\")\n",
    "print(f\"  Std:  {np.std(l1_per_active):.4f}\")\n",
    "print(f\"  Min:  {np.min(l1_per_active):.4f}\")\n",
    "print(f\"  Max:  {np.max(l1_per_active):.4f}\")\n",
    "\n",
    "print(f\"\\nPercentage of Features Active:\")\n",
    "print(f\"  Mean: {np.mean(percent_active):.2f}%\")\n",
    "print(f\"  Std:  {np.std(percent_active):.2f}%\")\n",
    "print(f\"  Min:  {np.min(percent_active):.2f}%\")\n",
    "print(f\"  Max:  {np.max(percent_active):.2f}%\")\n",
    "\n",
    "print(f\"\\n=== INDIVIDUAL OUTPUT DETAILS ===\")\n",
    "for i, stats in enumerate(individual_stats):\n",
    "    print(f\"Output {i+1}: L0={stats['l0_norm']:.2f}, L1={stats['l1_norm']:.4f}, \"\n",
    "          f\"L1/L0={stats['l1_per_active_feature']:.4f}, Active={stats['percent_active']:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
