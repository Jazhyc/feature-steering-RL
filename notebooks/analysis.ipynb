{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0034bdb",
   "metadata": {},
   "source": [
    "# FSRL Model Analysis\n",
    "\n",
    "This notebook analyzes trained Feature Steering RL models by comparing steered vs unsteered generation outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cea6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from fsrl import SAEAdapter, HookedModel\n",
    "from fsrl.utils.wandb_utils import WandBModelDownloader\n",
    "from fsrl.simPO.utils import apply_chat_template\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbf0e6",
   "metadata": {},
   "source": [
    "## Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891364af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['smart-dew-4', 'super-sea-2']\n"
     ]
    }
   ],
   "source": [
    "project_name = \"Gemma2-9B-muon\"\n",
    "downloader = WandBModelDownloader(entity=\"feature-steering-RL\", project=project_name)\n",
    "downloader.download_all_models(project_name)\n",
    "\n",
    "available_models = downloader.list_downloaded_models(project_name)\n",
    "print(f\"Available models: {available_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d3e940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: smart-dew-4\n"
     ]
    }
   ],
   "source": [
    "selected_model = 'smart-dew-4'\n",
    "print(f\"Using: {selected_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321fb69f",
   "metadata": {},
   "source": [
    "## Load Model and Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695ad12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23babab6b0e841f9a6db809dda10140f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b-it into HookedTransformer\n",
      "Loading adapter from: /lambda/nfs/fsrl/feature-steering-RL/models/Gemma2-9B-muon/smart-dew-4/adapter\n",
      "Adapter loaded from /lambda/nfs/fsrl/feature-steering-RL/models/Gemma2-9B-muon/smart-dew-4/adapter\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "base_model = HookedTransformer.from_pretrained_no_processing(\"google/gemma-2-9b-it\", device=\"cuda\", dtype=torch.bfloat16)\n",
    "adapter_path = downloader.models_base_dir / project_name / selected_model / \"adapter\"\n",
    "print(f\"Loading adapter from: {adapter_path}\")\n",
    "\n",
    "sae_adapter = SAEAdapter.load_from_pretrained_adapter(adapter_path, device=\"cuda\")\n",
    "hooked_model = HookedModel(base_model, sae_adapter)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0042e7",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867ed752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ae0ea25e404d93ba1aedbfbf912537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f799ccf9f1424a4fb45e5bfc89280c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/203M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7377f5152e0a482d95a6717d0f22c55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/203M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb218faa89b04feb89cd77c301a3cc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/13.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496cc93d23ad4d2a8edf8611399e4664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/59876 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c4efec1cc44fce92cfb4f891ae7e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"princeton-nlp/llama3-ultrafeedback-armorm\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91bd41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "num_samples = 5\n",
    "\n",
    "# Get 5 examples with shortest prompts\n",
    "df = pd.DataFrame(dataset)\n",
    "df['prompt_length'] = df['prompt'].str.len()\n",
    "shortest_examples = df.nsmallest(num_samples, 'prompt_length')[['prompt', 'chosen', 'rejected']]\n",
    "\n",
    "# Define the Gemma2 chat template (from config)\n",
    "gemma2_chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] | trim + '\\n\\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + content | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "\n",
    "def format_with_chat_template(example, tokenizer, chat_template):\n",
    "    \"\"\"Format an example using the chat template.\"\"\"\n",
    "    # Set the chat template\n",
    "    tokenizer.chat_template = chat_template\n",
    "    \n",
    "    # Apply chat template to get properly formatted text\n",
    "    formatted_example = apply_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=tokenizer,\n",
    "        chat_template=chat_template,\n",
    "        task=\"simpo\",\n",
    "        auto_insert_empty_system_msg=True\n",
    "    )\n",
    "    \n",
    "    return formatted_example\n",
    "\n",
    "def format_for_generation(example, tokenizer, chat_template):\n",
    "    \"\"\"Format an example for generation with generation prompt.\"\"\"\n",
    "    # Set the chat template\n",
    "    tokenizer.chat_template = chat_template\n",
    "    \n",
    "    # Apply chat template to get properly formatted text with generation prompt\n",
    "    formatted_example = apply_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=tokenizer,\n",
    "        chat_template=chat_template,\n",
    "        task=\"simpo_generation\",\n",
    "        auto_insert_empty_system_msg=True\n",
    "    )\n",
    "    \n",
    "    return formatted_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec93406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom prompts (formatted):\n",
      "\n",
      "Example 1:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "I have 3 apples and give 3 away. What do I have now?<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "Example 2:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "What is the meaning of life?<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "Example 3:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "Explain the theory of relativity in simple terms.<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create custom examples to test\n",
    "custom_prompts = [\n",
    "    \"I have 3 apples and give 3 away. What do I have now?\",\n",
    "    \"What is the meaning of life?\",\n",
    "    \"Explain the theory of relativity in simple terms.\",\n",
    "]\n",
    "\n",
    "# Convert custom prompts to proper OpenAI format expected by simpo task\n",
    "custom_examples = []\n",
    "for prompt in custom_prompts:\n",
    "    # For simpo task, we need chosen and rejected as conversation histories\n",
    "    # Since we don't have real chosen/rejected responses, we'll create minimal examples\n",
    "    example = {\n",
    "        'chosen': [\n",
    "            {'role': 'user', 'content': prompt},\n",
    "            {'role': 'assistant', 'content': ''}  # Empty response - we'll generate this\n",
    "        ],\n",
    "        'rejected': [\n",
    "            {'role': 'user', 'content': prompt},\n",
    "            {'role': 'assistant', 'content': ''}  # Empty response - we'll generate this  \n",
    "        ],\n",
    "        'prompt': prompt,\n",
    "    }\n",
    "    custom_examples.append(example)\n",
    "\n",
    "# Format examples using the chat template\n",
    "formatted_examples = []\n",
    "for i, example in enumerate(custom_examples):\n",
    "    formatted_example = format_with_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=base_model.tokenizer,\n",
    "        chat_template=gemma2_chat_template\n",
    "    )\n",
    "    formatted_examples.append(formatted_example)\n",
    "\n",
    "print(\"Custom prompts (formatted):\")\n",
    "for i, example in enumerate(formatted_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"PROMPT:\")\n",
    "    print(example['text_prompt'])\n",
    "    # Note: chosen/rejected will be empty for custom prompts\n",
    "    if example.get('text_chosen'):\n",
    "        print(\"\\nCHOSEN:\")\n",
    "        print(example['text_chosen'])\n",
    "    if example.get('text_rejected'):\n",
    "        print(\"\\nREJECTED:\")\n",
    "        print(example['text_rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a224c",
   "metadata": {},
   "source": [
    "## Generate Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88d5e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating unsteered response for prompt 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating steered response for prompt 1...\n",
      "Processed custom prompt 1/3\n",
      "Generating unsteered response for prompt 2...\n",
      "Generating steered response for prompt 2...\n",
      "Processed custom prompt 2/3\n",
      "Generating unsteered response for prompt 3...\n",
      "Generating steered response for prompt 3...\n",
      "Processed custom prompt 3/3\n",
      "\n",
      "All outputs saved to: outputs/custom_prompts_comparison.txt\n",
      "Generated 3 steered outputs for analysis\n",
      "\n",
      "================================================================================\n",
      "RESULTS PREVIEW:\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Custom Prompt 1\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "\"I have 3 apples and give 3 away. What do I have now?\"\n",
      "\n",
      "FORMATTED PROMPT (for generation):\n",
      "<bos><start_of_turn>user\n",
      "I have 3 apples and give 3 away. What do I have now?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "UNSTEERED OUTPUT:\n",
      "user\n",
      "I have 3 apples and give 3 away. What do I have now?\n",
      "model\n",
      "You have **zero** apples. \n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEERED OUTPUT:\n",
      "user\n",
      "I have 3 apples and give 3 away. What do I have now?\n",
      "model\n",
      "You﻿/* WT(#Recursion﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/* Debug﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*̣t﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/* Zellen﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/* levure<unused28>﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/* WP☆☆﻿/*﻿/*﻿/*﻿/*﻿/* /*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*ENDED﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/* OTHER﻿/**﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Custom Prompt 2\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "\"What is the meaning of life?\"\n",
      "\n",
      "FORMATTED PROMPT (for generation):\n",
      "<bos><start_of_turn>user\n",
      "What is the meaning of life?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "UNSTEERED OUTPUT:\n",
      "user\n",
      "What is the meaning of life?\n",
      "model\n",
      "As an AI, I don't have personal beliefs or experiences to draw on, so I can't tell you the meaning of life. That's a question philosophers have been pondering for centuries!\n",
      "\n",
      "However, I can offer some perspectives:\n",
      "\n",
      "* **There is no one answer:** The meaning of life is often personal and unique to each individual. What gives life meaning for one person might be different for another.\n",
      "* **It's about the journey, not the destination:** Many believe that life is about the experiences we have, the relationships we build, and the impact we make on the world, rather than reaching a specific end goal.\n",
      "* **Finding your purpose:** Some people find meaning in pursuing their passions, helping others, or making a difference in their community.\n",
      "* **Appreciating the present moment:** Living in the present and finding joy in the small things can also be a source of meaning.\n",
      "\n",
      "Ultimately, the meaning of life is up to you to decide. What do *you* think gives your life meaning?\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEERED OUTPUT:\n",
      "user\n",
      "What is the meaning of life?\n",
      "model\n",
      "﻿/*﻿/* Brendan不是﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/* utafitiHapana﻿/*﻿/* slavuman﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Custom Prompt 3\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "\"Explain the theory of relativity in simple terms.\"\n",
      "\n",
      "FORMATTED PROMPT (for generation):\n",
      "<bos><start_of_turn>user\n",
      "Explain the theory of relativity in simple terms.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "UNSTEERED OUTPUT:\n",
      "user\n",
      "Explain the theory of relativity in simple terms.\n",
      "model\n",
      "Imagine you're throwing a ball on a train. To you, it looks like the ball is moving at a normal speed. But to someone standing outside the train, the ball is moving much faster because it's also moving with the speed of the train.  \n",
      "\n",
      "That's kind of like **special relativity**. It says that **motion is relative**. How fast something seems to be moving depends on who is observing it.\n",
      "\n",
      "Here are some key ideas:\n",
      "\n",
      "* **The speed of light is constant:** No matter how fast you're moving, light always travels at the same speed. This seems weird, but it's true!\n",
      "* **Time is relative:** Because the speed of light is constant, time can actually slow down for objects that are moving very fast. This is called time dilation.\n",
      "* **Space is relative:** Similarly, space can also appear to shrink for objects that are moving very fast. This is called length contraction.\n",
      "\n",
      "**General relativity** takes it a step further. It says that **gravity isn't a force, but a curvature of space and time**. \n",
      "\n",
      "Imagine a bowling ball placed on a trampoline. It creates a dip, right? That's kind of how massive objects warp space and time around them\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEERED OUTPUT:\n",
      "user\n",
      "Explain the theory of relativity in simple terms.\n",
      "model\n",
      "Get﻿/*ded﻿/*﻿/* working﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/* KrasMind﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*﻿/*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 256\n",
    "\n",
    "steered_outputs = []\n",
    "output_buffer = []  # Buffer to collect all output text\n",
    "\n",
    "for i, example in enumerate(formatted_examples):\n",
    "    # Format the same example for generation (with generation prompt)\n",
    "    custom_example = custom_examples[i]  # Use custom_examples instead of shortest_examples\n",
    "    generation_example = format_for_generation(\n",
    "        example=custom_example,\n",
    "        tokenizer=base_model.tokenizer,\n",
    "        chat_template=gemma2_chat_template\n",
    "    )\n",
    "    generation_prompt = generation_example['text_prompt']\n",
    "    \n",
    "    buffer_text = f\"\\n{'='*80}\\n\"\n",
    "    buffer_text += f\"Custom Prompt {i+1}\\n\"\n",
    "    buffer_text += f\"{'='*80}\\n\\n\"\n",
    "    \n",
    "    buffer_text += \"ORIGINAL PROMPT:\\n\"\n",
    "    buffer_text += f'\"{custom_prompts[i]}\"\\n\\n'\n",
    "    \n",
    "    buffer_text += \"FORMATTED PROMPT (for generation):\\n\"\n",
    "    buffer_text += generation_prompt + \"\\n\"\n",
    "    \n",
    "    # Generate without steering\n",
    "    print(f\"Generating unsteered response for prompt {i+1}...\")\n",
    "    hooked_model.disable_steering()\n",
    "    unsteered_output = hooked_model.generate(generation_prompt, max_new_tokens=max_new_tokens, verbose=False)\n",
    "    buffer_text += \"\\n\" + \"-\"*60 + \"\\n\"\n",
    "    buffer_text += \"UNSTEERED OUTPUT:\\n\"\n",
    "    buffer_text += unsteered_output + \"\\n\"\n",
    "    \n",
    "    # Generate with steering\n",
    "    print(f\"Generating steered response for prompt {i+1}...\")\n",
    "    hooked_model.enable_steering()\n",
    "    steered_output = hooked_model.generate(generation_prompt, max_new_tokens=max_new_tokens, verbose=False)\n",
    "    buffer_text += \"\\n\" + \"-\"*60 + \"\\n\"\n",
    "    buffer_text += \"STEERED OUTPUT:\\n\"\n",
    "    buffer_text += steered_output + \"\\n\"\n",
    "\n",
    "    steered_outputs.append(steered_output)\n",
    "    output_buffer.append(buffer_text)\n",
    "    \n",
    "    print(f\"Processed custom prompt {i+1}/{len(formatted_examples)}\")\n",
    "\n",
    "# Write all outputs to a text file\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create outputs directory if it doesn't exist\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Simple filename that will be overwritten each run\n",
    "filename = \"outputs/custom_prompts_comparison.txt\"\n",
    "\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"FSRL Model Analysis - Custom Prompts Comparison\\n\")\n",
    "    f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Model: {selected_model}\\n\")\n",
    "    f.write(f\"Max new tokens: {max_new_tokens}\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\\n\")\n",
    "    \n",
    "    for buffer_text in output_buffer:\n",
    "        f.write(buffer_text)\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {filename}\")\n",
    "print(f\"Generated {len(steered_outputs)} steered outputs for analysis\")\n",
    "\n",
    "# Also display the results in the notebook\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS PREVIEW:\")\n",
    "print(\"=\"*80)\n",
    "for i, buffer_text in enumerate(output_buffer):\n",
    "    print(buffer_text)\n",
    "    if i < len(output_buffer) - 1:  # Don't add extra separator after last item\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea54e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_l0_norm(tensor):\n",
    "    \"\"\"\n",
    "    Calculate the average L0 norm of a tensor.\n",
    "    \n",
    "    L0 norm counts the number of non-zero elements.\n",
    "    For a tensor with multiple dimensions, we calculate the L0 norm\n",
    "    across the feature dimension and then average across other dimensions.\n",
    "    \n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape (batch, seq_len, features) or similar\n",
    "        \n",
    "    Returns:\n",
    "        float: Average L0 norm (average number of non-zero features)\n",
    "    \"\"\"\n",
    "    # Count non-zero elements across the last dimension (features)\n",
    "    l0_norms = torch.count_nonzero(tensor, dim=-1).float()\n",
    "    \n",
    "    # Average across all other dimensions\n",
    "    avg_l0_norm = l0_norms.mean().item()\n",
    "    \n",
    "    return avg_l0_norm\n",
    "\n",
    "def calculate_average_l1_norm(tensor):\n",
    "    \"\"\"\n",
    "    Calculate the average L1 norm of a tensor.\n",
    "    \n",
    "    L1 norm is the sum of absolute values of elements.\n",
    "    For a tensor with multiple dimensions, we calculate the L1 norm\n",
    "    across the feature dimension and then average across other dimensions.\n",
    "    \n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape (batch, seq_len, features) or similar\n",
    "        \n",
    "    Returns:\n",
    "        float: Average L1 norm (average sum of absolute values)\n",
    "    \"\"\"\n",
    "    # Calculate L1 norm across the last dimension (features)\n",
    "    l1_norms = torch.sum(torch.abs(tensor), dim=-1)\n",
    "    \n",
    "    # Average across all other dimensions\n",
    "    avg_l1_norm = l1_norms.mean().item()\n",
    "    \n",
    "    return avg_l1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb9ebc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing output 1/3...\n",
      "  Shape: torch.Size([1, 279, 16384])\n",
      "  L0 norm: 58.52, L1 norm: 118.5000\n",
      "Processing output 2/3...\n",
      "  Shape: torch.Size([1, 269, 16384])\n",
      "  L0 norm: 50.27, L1 norm: 121.0000\n",
      "Processing output 3/3...\n",
      "  Shape: torch.Size([1, 271, 16384])\n",
      "  L0 norm: 52.61, L1 norm: 120.0000\n",
      "\n",
      "Processed 3 outputs\n"
     ]
    }
   ],
   "source": [
    "# Analyze activations for each output individually\n",
    "individual_stats = []\n",
    "\n",
    "for i, output in enumerate(steered_outputs):\n",
    "    print(f\"Processing output {i+1}/{len(steered_outputs)}...\")\n",
    "    \n",
    "    # Get activations for this specific output\n",
    "    logits, cache = hooked_model.run_with_cache(output)\n",
    "    sae_activations = cache['blocks.12.hook_resid_post.hook_sae_adapter']\n",
    "    \n",
    "    # Calculate norms for this output\n",
    "    l0_norm = calculate_average_l0_norm(sae_activations)\n",
    "    l1_norm = calculate_average_l1_norm(sae_activations)\n",
    "    \n",
    "    # Store individual statistics\n",
    "    stats = {\n",
    "        'output_idx': i,\n",
    "        'shape': sae_activations.shape,\n",
    "        'l0_norm': l0_norm,\n",
    "        'l1_norm': l1_norm,\n",
    "        'l1_per_active_feature': l1_norm / l0_norm if l0_norm > 0 else 0,\n",
    "        'percent_active': (l0_norm / sae_activations.shape[-1]) * 100\n",
    "    }\n",
    "    individual_stats.append(stats)\n",
    "    \n",
    "    print(f\"  Shape: {sae_activations.shape}\")\n",
    "    print(f\"  L0 norm: {l0_norm:.2f}, L1 norm: {l1_norm:.4f}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(individual_stats)} outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af92178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AGGREGATE STATISTICS ===\n",
      "Number of outputs analyzed: 3\n",
      "Total features per output: 16384\n",
      "\n",
      "L0 Norm (Sparsity) Statistics:\n",
      "  Mean: 53.80\n",
      "  Std:  3.47\n",
      "  Min:  50.27\n",
      "  Max:  58.52\n",
      "\n",
      "L1 Norm Statistics:\n",
      "  Mean: 119.8333\n",
      "  Std:  1.0274\n",
      "  Min:  118.5000\n",
      "  Max:  121.0000\n",
      "\n",
      "L1 per Active Feature Statistics:\n",
      "  Mean: 2.2377\n",
      "  Std:  0.1590\n",
      "  Min:  2.0250\n",
      "  Max:  2.4071\n",
      "\n",
      "Percentage of Features Active:\n",
      "  Mean: 0.33%\n",
      "  Std:  0.02%\n",
      "  Min:  0.31%\n",
      "  Max:  0.36%\n",
      "\n",
      "=== INDIVIDUAL OUTPUT DETAILS ===\n",
      "Output 1: L0=58.52, L1=118.5000, L1/L0=2.0250, Active=0.4%\n",
      "Output 2: L0=50.27, L1=121.0000, L1/L0=2.4071, Active=0.3%\n",
      "Output 3: L0=52.61, L1=120.0000, L1/L0=2.2811, Active=0.3%\n"
     ]
    }
   ],
   "source": [
    "# Compute aggregate statistics from individual outputs\n",
    "import numpy as np\n",
    "\n",
    "l0_norms = [stats['l0_norm'] for stats in individual_stats]\n",
    "l1_norms = [stats['l1_norm'] for stats in individual_stats]\n",
    "l1_per_active = [stats['l1_per_active_feature'] for stats in individual_stats]\n",
    "percent_active = [stats['percent_active'] for stats in individual_stats]\n",
    "\n",
    "print(\"=== AGGREGATE STATISTICS ===\")\n",
    "print(f\"Number of outputs analyzed: {len(individual_stats)}\")\n",
    "\n",
    "# Get feature count (should be same for all)\n",
    "total_features = individual_stats[0]['shape'][-1]\n",
    "print(f\"Total features per output: {total_features}\")\n",
    "\n",
    "print(f\"\\nL0 Norm (Sparsity) Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l0_norms):.2f}\")\n",
    "print(f\"  Std:  {np.std(l0_norms):.2f}\")\n",
    "print(f\"  Min:  {np.min(l0_norms):.2f}\")\n",
    "print(f\"  Max:  {np.max(l0_norms):.2f}\")\n",
    "\n",
    "print(f\"\\nL1 Norm Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l1_norms):.4f}\")\n",
    "print(f\"  Std:  {np.std(l1_norms):.4f}\")\n",
    "print(f\"  Min:  {np.min(l1_norms):.4f}\")\n",
    "print(f\"  Max:  {np.max(l1_norms):.4f}\")\n",
    "\n",
    "print(f\"\\nL1 per Active Feature Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l1_per_active):.4f}\")\n",
    "print(f\"  Std:  {np.std(l1_per_active):.4f}\")\n",
    "print(f\"  Min:  {np.min(l1_per_active):.4f}\")\n",
    "print(f\"  Max:  {np.max(l1_per_active):.4f}\")\n",
    "\n",
    "print(f\"\\nPercentage of Features Active:\")\n",
    "print(f\"  Mean: {np.mean(percent_active):.2f}%\")\n",
    "print(f\"  Std:  {np.std(percent_active):.2f}%\")\n",
    "print(f\"  Min:  {np.min(percent_active):.2f}%\")\n",
    "print(f\"  Max:  {np.max(percent_active):.2f}%\")\n",
    "\n",
    "print(f\"\\n=== INDIVIDUAL OUTPUT DETAILS ===\")\n",
    "for i, stats in enumerate(individual_stats):\n",
    "    print(f\"Output {i+1}: L0={stats['l0_norm']:.2f}, L1={stats['l1_norm']:.4f}, \"\n",
    "          f\"L1/L0={stats['l1_per_active_feature']:.4f}, Active={stats['percent_active']:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
