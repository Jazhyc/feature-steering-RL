{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0034bdb",
   "metadata": {},
   "source": [
    "# FSRL Model Analysis\n",
    "\n",
    "This notebook analyzes trained Feature Steering RL models by comparing steered vs unsteered generation outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cea6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from fsrl import SAEAdapter, HookedModel\n",
    "from fsrl.utils.wandb_utils import WandBModelDownloader\n",
    "from fsrl.simPO.utils import apply_chat_template\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbf0e6",
   "metadata": {},
   "source": [
    "## Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891364af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['ethereal-salad-14', 'fragrant-armadillo-22', 'fragrant-bush-43', 'fragrant-night-24', 'giddy-valley-48', 'hearty-wildflower-1', 'icy-plasma-38', 'likely-meadow-40', 'peach-dew-23', 'pleasant-serenity-21', 'prime-jazz-47', 'rosy-glitter-39', 'winter-planet-42']\n"
     ]
    }
   ],
   "source": [
    "downloader = WandBModelDownloader(entity=\"feature-steering-RL\", project=\"Gemma2-2B\")\n",
    "downloader.download_all_models(\"Gemma2-2B\")\n",
    "\n",
    "available_models = downloader.list_downloaded_models(\"Gemma2-2B\")\n",
    "print(f\"Available models: {available_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d3e940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: giddy-valley-48\n"
     ]
    }
   ],
   "source": [
    "selected_model = 'giddy-valley-48'\n",
    "print(f\"Using: {selected_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321fb69f",
   "metadata": {},
   "source": [
    "## Load Model and Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695ad12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e4462c6324425b9d50d34cfebfb9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b-it into HookedTransformer\n",
      "Loading adapter from: /home/jazhyc/projects/FSRL/feature-steering-RL/models/Gemma2-2B/giddy-valley-48/adapter\n",
      "Adapter loaded from /home/jazhyc/projects/FSRL/feature-steering-RL/models/Gemma2-2B/giddy-valley-48/adapter\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "base_model = HookedTransformer.from_pretrained(\"google/gemma-2-2b-it\", device=\"cuda\", dtype=torch.bfloat16)\n",
    "adapter_path = downloader.models_base_dir / \"Gemma2-2B\" / selected_model / \"adapter\"\n",
    "print(f\"Loading adapter from: {adapter_path}\")\n",
    "\n",
    "sae_adapter = SAEAdapter.load_from_pretrained_adapter(adapter_path, device=\"cuda\")\n",
    "hooked_model = HookedModel(base_model, sae_adapter)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0042e7",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867ed752",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"princeton-nlp/llama3-ultrafeedback-armorm\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91bd41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "num_samples = 5\n",
    "\n",
    "# Get 5 examples with shortest prompts\n",
    "df = pd.DataFrame(dataset)\n",
    "df['prompt_length'] = df['prompt'].str.len()\n",
    "shortest_examples = df.nsmallest(num_samples, 'prompt_length')[['prompt', 'chosen', 'rejected']]\n",
    "\n",
    "# Define the Gemma2 chat template (from config)\n",
    "gemma2_chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] | trim + '\\n\\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + content | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "\n",
    "def format_with_chat_template(example, tokenizer, chat_template):\n",
    "    \"\"\"Format an example using the chat template.\"\"\"\n",
    "    # Set the chat template\n",
    "    tokenizer.chat_template = chat_template\n",
    "    \n",
    "    # Apply chat template to get properly formatted text\n",
    "    formatted_example = apply_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=tokenizer,\n",
    "        chat_template=chat_template,\n",
    "        task=\"simpo\",\n",
    "        auto_insert_empty_system_msg=True\n",
    "    )\n",
    "    \n",
    "    return formatted_example\n",
    "\n",
    "def format_for_generation(example, tokenizer, chat_template):\n",
    "    \"\"\"Format an example for generation with generation prompt.\"\"\"\n",
    "    # Set the chat template\n",
    "    tokenizer.chat_template = chat_template\n",
    "    \n",
    "    # Apply chat template to get properly formatted text with generation prompt\n",
    "    formatted_example = apply_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=tokenizer,\n",
    "        chat_template=chat_template,\n",
    "        task=\"simpo_generation\",\n",
    "        auto_insert_empty_system_msg=True\n",
    "    )\n",
    "    \n",
    "    return formatted_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec93406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples with shortest prompts (formatted):\n",
      "\n",
      "Example 1:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "- Knife<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "A simple yet versatile tool!\n",
      "\n",
      "Here are some interesting facts about knives:\n",
      "\n",
      "1. **The first knives**: The earliest known knives date back to around 2.5 million years ago, during the Oldowan period. These early knives were made from stone and had a sharp edge.\n",
      "2. **Materials**: Knives can be made from a variety of materials, including metal (e.g., steel, titanium), plastic, wood, bone, and even stone.\n",
      "3. **Types**: There are many types of knives, including:\n",
      "\t* Chef's knife (for cooking)\n",
      "\t* Pocket knife (for everyday carry)\n",
      "\t* Hunting knife (for hunting and skinning)\n",
      "\t* Tactical knife (for self-defense)\n",
      "\t* Utility knife (for general-purpose use)\n",
      "4. **Sharpness**: A sharp knife is safer and more effective than a dull one. A sharp knife can be honed or sharpened to maintain its edge.\n",
      "5. **Handling**: Proper handling is important when using a knife. Always cut away from your body, and keep your fingers curled under and out of the way of the blade.\n",
      "6. **History**: Knives have been an essential tool for humans throughout history, used for food preparation, self-defense, and many other purposes.\n",
      "7. **Collecting**: Some people collect knives as a hobby, seeking out rare or unique specimens.\n",
      "\n",
      "Do you have a favorite type of knife or a special use for knives?<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "A sharp tool!<end_of_turn>\n",
      "\n",
      "\n",
      "Example 2:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "12 / 4 - 3 * 5=<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "To evaluate this expression, we need to follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Divide 12 by 4: 12 / 4 = 3\n",
      "2. Multiply 3 by 5: 3 * 5 = 15\n",
      "3. Subtract 15 from 3: 3 - 15 = -12\n",
      "\n",
      "So, 12 / 4 - 3 * 5 = -12.<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "To evaluate this expression, we need to follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Divide 12 by 4: 12 / 4 = 3\n",
      "2. Multiply 3 by 5: 3 * 5 = 15\n",
      "3. Subtract 15 from 3: 3 - 15 = -12\n",
      "\n",
      "So the final answer is -12.<end_of_turn>\n",
      "\n",
      "\n",
      "Example 3:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "How do rats fly?<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "I think there may be some confusion here! Rats do not actually fly. Rats are rodents that live on the ground and do not possess the ability to fly. They have four legs and a sturdy body, but they do not have wings or any other features that would allow them to defy gravity and take to the air.\n",
      "\n",
      "It's possible that you may be thinking of a different animal that can fly, such as birds, bats, or insects. Could you be thinking of one of those?<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "I think there may be some confusion here! Rats don't actually fly. They are mammals that live on the ground and don't have the ability to fly. Rats are rodents that are typically found in a variety of environments, including homes, farms, and forests, but they are not capable of flight.\n",
      "\n",
      "If you're thinking of an animal that can fly, there are many examples of insects, birds, and even some mammals like bats and gliders that are capable of flight. But rats are not among them.\n",
      "\n",
      "Let me know if you have any other questions or if there's anything else I can help clarify!<end_of_turn>\n",
      "\n",
      "\n",
      "Example 4:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "(7 - 2) ÷ 3 + 4=<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "Let's follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Evaluate the expression inside the parentheses: 7 - 2 = 5\n",
      "2. Divide the result by 3: 5 ÷ 3 = 1.67 (approx)\n",
      "3. Add 4 to the result: 1.67 + 4 = 5.67 (approx)\n",
      "\n",
      "So, the final answer is approximately 5.67.<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "To evaluate this expression, we need to follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Evaluate the expression inside the parentheses: 7 - 2 = 5\n",
      "2. Divide the result by 3: 5 ÷ 3 = 1.67\n",
      "3. Add 4 to the result: 1.67 + 4 = 5.67\n",
      "\n",
      "So, (7 - 2) ÷ 3 + 4 = 5.67.<end_of_turn>\n",
      "\n",
      "\n",
      "Example 5:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "7 - (4 ÷ 2) + 9=<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "To evaluate this expression, we need to follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Divide 4 by 2: 4 ÷ 2 = 2\n",
      "2. Subtract 2 from 7: 7 - 2 = 5\n",
      "3. Add 9 to 5: 5 + 9 = 14\n",
      "\n",
      "So, 7 - (4 ÷ 2) + 9 = 14.<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "To evaluate this expression, we need to follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Divide 4 by 2: 4 ÷ 2 = 2\n",
      "2. Subtract 2 from 7: 7 - 2 = 5\n",
      "3. Add 9 to 5: 5 + 9 = 14\n",
      "\n",
      "So the final answer is 14.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Format examples using the chat template\n",
    "formatted_examples = []\n",
    "for i, row in shortest_examples.iterrows():\n",
    "    formatted_example = format_with_chat_template(\n",
    "        example=row.to_dict(),\n",
    "        tokenizer=base_model.tokenizer,\n",
    "        chat_template=gemma2_chat_template\n",
    "    )\n",
    "    formatted_examples.append(formatted_example)\n",
    "\n",
    "print(\"Examples with shortest prompts (formatted):\")\n",
    "for i, example in enumerate(formatted_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"PROMPT:\")\n",
    "    print(example['text_prompt'])\n",
    "    print(\"\\nCHOSEN:\")\n",
    "    print(example['text_chosen'])\n",
    "    print(\"\\nREJECTED:\")\n",
    "    print(example['text_rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a224c",
   "metadata": {},
   "source": [
    "## Generate Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88d5e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed example 1/5\n",
      "Processed example 2/5\n",
      "Processed example 3/5\n",
      "Processed example 4/5\n",
      "Processed example 5/5\n",
      "\n",
      "All outputs saved to: outputs/generation_comparison.txt\n",
      "Generated 5 steered outputs for analysis\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 256\n",
    "\n",
    "steered_outputs = []\n",
    "output_buffer = []  # Buffer to collect all output text\n",
    "\n",
    "for i, example in enumerate(formatted_examples):\n",
    "    # Format the same example for generation (with generation prompt)\n",
    "    row_dict = shortest_examples.iloc[i].to_dict()\n",
    "    generation_example = format_for_generation(\n",
    "        example=row_dict,\n",
    "        tokenizer=base_model.tokenizer,\n",
    "        chat_template=gemma2_chat_template\n",
    "    )\n",
    "    generation_prompt = generation_example['text_prompt']\n",
    "    \n",
    "    buffer_text = f\"\\n{'='*80}\\n\"\n",
    "    buffer_text += f\"Example {i+1}\\n\"\n",
    "    buffer_text += f\"{'='*80}\\n\\n\"\n",
    "    \n",
    "    buffer_text += \"PROMPT (for generation):\\n\"\n",
    "    buffer_text += generation_prompt + \"\\n\"\n",
    "    \n",
    "    buffer_text += \"\\n\" + \"-\"*60 + \"\\n\"\n",
    "    buffer_text += \"CHOSEN (from dataset):\\n\"\n",
    "    buffer_text += example['text_chosen'] + \"\\n\"\n",
    "    \n",
    "    # Generate without steering\n",
    "    hooked_model.disable_steering()\n",
    "    unsteered_output = hooked_model.generate(generation_prompt, max_new_tokens=max_new_tokens, verbose=False)\n",
    "    buffer_text += \"\\n\" + \"-\"*60 + \"\\n\"\n",
    "    buffer_text += \"UNSTEERED:\\n\"\n",
    "    buffer_text += unsteered_output + \"\\n\"\n",
    "    \n",
    "    # Generate with steering\n",
    "    hooked_model.enable_steering()\n",
    "    steered_output = hooked_model.generate(generation_prompt, max_new_tokens=max_new_tokens, verbose=False)\n",
    "    buffer_text += \"\\n\" + \"-\"*60 + \"\\n\"\n",
    "    buffer_text += \"STEERED:\\n\"\n",
    "    buffer_text += steered_output + \"\\n\"\n",
    "\n",
    "    steered_outputs.append(steered_output)\n",
    "    output_buffer.append(buffer_text)\n",
    "    \n",
    "    print(f\"Processed example {i+1}/{len(formatted_examples)}\")\n",
    "\n",
    "# Write all outputs to a text file\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create outputs directory if it doesn't exist\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Simple filename that will be overwritten each run\n",
    "filename = \"outputs/generation_comparison.txt\"\n",
    "\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"FSRL Model Analysis - Generation Comparisons\\n\")\n",
    "    f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Model: {selected_model}\\n\")\n",
    "    f.write(f\"Max new tokens: {max_new_tokens}\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\\n\")\n",
    "    \n",
    "    for buffer_text in output_buffer:\n",
    "        f.write(buffer_text)\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {filename}\")\n",
    "print(f\"Generated {len(steered_outputs)} steered outputs for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea54e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_l0_norm(tensor):\n",
    "    \"\"\"\n",
    "    Calculate the average L0 norm of a tensor.\n",
    "    \n",
    "    L0 norm counts the number of non-zero elements.\n",
    "    For a tensor with multiple dimensions, we calculate the L0 norm\n",
    "    across the feature dimension and then average across other dimensions.\n",
    "    \n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape (batch, seq_len, features) or similar\n",
    "        \n",
    "    Returns:\n",
    "        float: Average L0 norm (average number of non-zero features)\n",
    "    \"\"\"\n",
    "    # Count non-zero elements across the last dimension (features)\n",
    "    l0_norms = torch.count_nonzero(tensor, dim=-1).float()\n",
    "    \n",
    "    # Average across all other dimensions\n",
    "    avg_l0_norm = l0_norms.mean().item()\n",
    "    \n",
    "    return avg_l0_norm\n",
    "\n",
    "def calculate_average_l1_norm(tensor):\n",
    "    \"\"\"\n",
    "    Calculate the average L1 norm of a tensor.\n",
    "    \n",
    "    L1 norm is the sum of absolute values of elements.\n",
    "    For a tensor with multiple dimensions, we calculate the L1 norm\n",
    "    across the feature dimension and then average across other dimensions.\n",
    "    \n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape (batch, seq_len, features) or similar\n",
    "        \n",
    "    Returns:\n",
    "        float: Average L1 norm (average sum of absolute values)\n",
    "    \"\"\"\n",
    "    # Calculate L1 norm across the last dimension (features)\n",
    "    l1_norms = torch.sum(torch.abs(tensor), dim=-1)\n",
    "    \n",
    "    # Average across all other dimensions\n",
    "    avg_l1_norm = l1_norms.mean().item()\n",
    "    \n",
    "    return avg_l1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb9ebc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing output 1/5...\n",
      "  Shape: torch.Size([1, 264, 65536])\n",
      "  L0 norm: 65536.00, L1 norm: 378.8270\n",
      "Processing output 2/5...\n",
      "  Shape: torch.Size([1, 94, 65536])\n",
      "  L0 norm: 65536.00, L1 norm: 503.5284\n",
      "Processing output 3/5...\n",
      "  Shape: torch.Size([1, 130, 65536])\n",
      "  L0 norm: 65535.99, L1 norm: 414.1078\n",
      "Processing output 4/5...\n",
      "  Shape: torch.Size([1, 84, 65536])\n",
      "  L0 norm: 65536.00, L1 norm: 495.1365\n",
      "Processing output 5/5...\n",
      "  Shape: torch.Size([1, 94, 65536])\n",
      "  L0 norm: 65536.00, L1 norm: 525.3712\n",
      "\n",
      "Processed 5 outputs\n"
     ]
    }
   ],
   "source": [
    "# Analyze activations for each output individually\n",
    "individual_stats = []\n",
    "\n",
    "for i, output in enumerate(steered_outputs):\n",
    "    print(f\"Processing output {i+1}/{len(steered_outputs)}...\")\n",
    "    \n",
    "    # Get activations for this specific output\n",
    "    logits, cache = hooked_model.run_with_cache(output)\n",
    "    sae_activations = cache['blocks.12.hook_resid_post.hook_sae_adapter']\n",
    "    \n",
    "    # Calculate norms for this output\n",
    "    l0_norm = calculate_average_l0_norm(sae_activations)\n",
    "    l1_norm = calculate_average_l1_norm(sae_activations)\n",
    "    \n",
    "    # Store individual statistics\n",
    "    stats = {\n",
    "        'output_idx': i,\n",
    "        'shape': sae_activations.shape,\n",
    "        'l0_norm': l0_norm,\n",
    "        'l1_norm': l1_norm,\n",
    "        'l1_per_active_feature': l1_norm / l0_norm if l0_norm > 0 else 0,\n",
    "        'percent_active': (l0_norm / sae_activations.shape[-1]) * 100\n",
    "    }\n",
    "    individual_stats.append(stats)\n",
    "    \n",
    "    print(f\"  Shape: {sae_activations.shape}\")\n",
    "    print(f\"  L0 norm: {l0_norm:.2f}, L1 norm: {l1_norm:.4f}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(individual_stats)} outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af92178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AGGREGATE STATISTICS ===\n",
      "Number of outputs analyzed: 5\n",
      "Total features per output: 65536\n",
      "\n",
      "L0 Norm (Sparsity) Statistics:\n",
      "  Mean: 65536.00\n",
      "  Std:  0.00\n",
      "  Min:  65535.99\n",
      "  Max:  65536.00\n",
      "\n",
      "L1 Norm Statistics:\n",
      "  Mean: 463.3942\n",
      "  Std:  56.6396\n",
      "  Min:  378.8270\n",
      "  Max:  525.3712\n",
      "\n",
      "L1 per Active Feature Statistics:\n",
      "  Mean: 0.0071\n",
      "  Std:  0.0009\n",
      "  Min:  0.0058\n",
      "  Max:  0.0080\n",
      "\n",
      "Percentage of Features Active:\n",
      "  Mean: 100.00%\n",
      "  Std:  0.00%\n",
      "  Min:  100.00%\n",
      "  Max:  100.00%\n",
      "\n",
      "=== INDIVIDUAL OUTPUT DETAILS ===\n",
      "Output 1: L0=65536.00, L1=378.8270, L1/L0=0.0058, Active=100.0%\n",
      "Output 2: L0=65536.00, L1=503.5284, L1/L0=0.0077, Active=100.0%\n",
      "Output 3: L0=65535.99, L1=414.1078, L1/L0=0.0063, Active=100.0%\n",
      "Output 4: L0=65536.00, L1=495.1365, L1/L0=0.0076, Active=100.0%\n",
      "Output 5: L0=65536.00, L1=525.3712, L1/L0=0.0080, Active=100.0%\n"
     ]
    }
   ],
   "source": [
    "# Compute aggregate statistics from individual outputs\n",
    "import numpy as np\n",
    "\n",
    "l0_norms = [stats['l0_norm'] for stats in individual_stats]\n",
    "l1_norms = [stats['l1_norm'] for stats in individual_stats]\n",
    "l1_per_active = [stats['l1_per_active_feature'] for stats in individual_stats]\n",
    "percent_active = [stats['percent_active'] for stats in individual_stats]\n",
    "\n",
    "print(\"=== AGGREGATE STATISTICS ===\")\n",
    "print(f\"Number of outputs analyzed: {len(individual_stats)}\")\n",
    "\n",
    "# Get feature count (should be same for all)\n",
    "total_features = individual_stats[0]['shape'][-1]\n",
    "print(f\"Total features per output: {total_features}\")\n",
    "\n",
    "print(f\"\\nL0 Norm (Sparsity) Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l0_norms):.2f}\")\n",
    "print(f\"  Std:  {np.std(l0_norms):.2f}\")\n",
    "print(f\"  Min:  {np.min(l0_norms):.2f}\")\n",
    "print(f\"  Max:  {np.max(l0_norms):.2f}\")\n",
    "\n",
    "print(f\"\\nL1 Norm Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l1_norms):.4f}\")\n",
    "print(f\"  Std:  {np.std(l1_norms):.4f}\")\n",
    "print(f\"  Min:  {np.min(l1_norms):.4f}\")\n",
    "print(f\"  Max:  {np.max(l1_norms):.4f}\")\n",
    "\n",
    "print(f\"\\nL1 per Active Feature Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l1_per_active):.4f}\")\n",
    "print(f\"  Std:  {np.std(l1_per_active):.4f}\")\n",
    "print(f\"  Min:  {np.min(l1_per_active):.4f}\")\n",
    "print(f\"  Max:  {np.max(l1_per_active):.4f}\")\n",
    "\n",
    "print(f\"\\nPercentage of Features Active:\")\n",
    "print(f\"  Mean: {np.mean(percent_active):.2f}%\")\n",
    "print(f\"  Std:  {np.std(percent_active):.2f}%\")\n",
    "print(f\"  Min:  {np.min(percent_active):.2f}%\")\n",
    "print(f\"  Max:  {np.max(percent_active):.2f}%\")\n",
    "\n",
    "print(f\"\\n=== INDIVIDUAL OUTPUT DETAILS ===\")\n",
    "for i, stats in enumerate(individual_stats):\n",
    "    print(f\"Output {i+1}: L0={stats['l0_norm']:.2f}, L1={stats['l1_norm']:.4f}, \"\n",
    "          f\"L1/L0={stats['l1_per_active_feature']:.4f}, Active={stats['percent_active']:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
