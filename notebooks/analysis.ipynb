{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0034bdb",
   "metadata": {},
   "source": [
    "# FSRL Model Analysis\n",
    "\n",
    "This notebook analyzes trained Feature Steering RL models by comparing steered vs unsteered generation outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cea6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from fsrl import SAEAdapter, HookedModel\n",
    "from fsrl.utils.wandb_utils import WandBModelDownloader\n",
    "from fsrl.simPO.utils import apply_chat_template\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbf0e6",
   "metadata": {},
   "source": [
    "## Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891364af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['drawn-paper-4', 'electric-pine-5', 'exalted-eon-7', 'logical-microwave-9', 'mild-glade-10', 'royal-valley-2', 'splendid-smoke-8', 'wobbly-shadow-6']\n"
     ]
    }
   ],
   "source": [
    "project_name = \"Gemma2-2B-clean\"\n",
    "downloader = WandBModelDownloader(entity=\"feature-steering-RL\", project=project_name)\n",
    "downloader.download_all_models(project_name)\n",
    "\n",
    "available_models = downloader.list_downloaded_models(project_name)\n",
    "print(f\"Available models: {available_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d3e940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: royal-valley-2\n"
     ]
    }
   ],
   "source": [
    "selected_model = 'royal-valley-2'\n",
    "print(f\"Using: {selected_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321fb69f",
   "metadata": {},
   "source": [
    "## Load Model and Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695ad12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e133fa3da44945bc14732a7de99040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b-it into HookedTransformer\n",
      "Loading adapter from: /home/jazhyc/projects/FSRL/feature-steering-RL/models/Gemma2-2B-clean/royal-valley-2/adapter\n",
      "Adapter loaded from /home/jazhyc/projects/FSRL/feature-steering-RL/models/Gemma2-2B-clean/royal-valley-2/adapter\n",
      "Model loaded!\n",
      "Adapter loaded from /home/jazhyc/projects/FSRL/feature-steering-RL/models/Gemma2-2B-clean/royal-valley-2/adapter\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "base_model = HookedTransformer.from_pretrained_no_processing(\"google/gemma-2-2b-it\", device=\"cuda\", dtype=torch.bfloat16)\n",
    "adapter_path = downloader.models_base_dir / project_name / selected_model / \"adapter\"\n",
    "print(f\"Loading adapter from: {adapter_path}\")\n",
    "\n",
    "sae_adapter = SAEAdapter.load_from_pretrained_adapter(adapter_path, device=\"cuda\")\n",
    "hooked_model = HookedModel(base_model, sae_adapter)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0042e7",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867ed752",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"princeton-nlp/llama3-ultrafeedback-armorm\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91bd41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "num_samples = 5\n",
    "\n",
    "# Get 5 examples with shortest prompts\n",
    "df = pd.DataFrame(dataset)\n",
    "df['prompt_length'] = df['prompt'].str.len()\n",
    "shortest_examples = df.nsmallest(num_samples, 'prompt_length')[['prompt', 'chosen', 'rejected']]\n",
    "\n",
    "# Define the Gemma2 chat template (from config)\n",
    "gemma2_chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] | trim + '\\n\\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + content | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "\n",
    "def format_with_chat_template(example, tokenizer, chat_template):\n",
    "    \"\"\"Format an example using the chat template.\"\"\"\n",
    "    # Set the chat template\n",
    "    tokenizer.chat_template = chat_template\n",
    "    \n",
    "    # Apply chat template to get properly formatted text\n",
    "    formatted_example = apply_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=tokenizer,\n",
    "        chat_template=chat_template,\n",
    "        task=\"simpo\",\n",
    "        auto_insert_empty_system_msg=True\n",
    "    )\n",
    "    \n",
    "    return formatted_example\n",
    "\n",
    "def format_for_generation(example, tokenizer, chat_template):\n",
    "    \"\"\"Format an example for generation with generation prompt.\"\"\"\n",
    "    # Set the chat template\n",
    "    tokenizer.chat_template = chat_template\n",
    "    \n",
    "    # Apply chat template to get properly formatted text with generation prompt\n",
    "    formatted_example = apply_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=tokenizer,\n",
    "        chat_template=chat_template,\n",
    "        task=\"simpo_generation\",\n",
    "        auto_insert_empty_system_msg=True\n",
    "    )\n",
    "    \n",
    "    return formatted_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bec93406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom prompts (formatted):\n",
      "\n",
      "Example 1:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "I have 3 apples and give 3 away. What do I have now?<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create custom examples to test\n",
    "custom_prompts = [\n",
    "    \"I have 3 apples and give 3 away. What do I have now?\"\n",
    "]\n",
    "\n",
    "# Convert custom prompts to proper OpenAI format expected by simpo task\n",
    "custom_examples = []\n",
    "for prompt in custom_prompts:\n",
    "    # For simpo task, we need chosen and rejected as conversation histories\n",
    "    # Since we don't have real chosen/rejected responses, we'll create minimal examples\n",
    "    example = {\n",
    "        'chosen': [\n",
    "            {'role': 'user', 'content': prompt},\n",
    "            {'role': 'assistant', 'content': ''}  # Empty response - we'll generate this\n",
    "        ],\n",
    "        'rejected': [\n",
    "            {'role': 'user', 'content': prompt},\n",
    "            {'role': 'assistant', 'content': ''}  # Empty response - we'll generate this  \n",
    "        ],\n",
    "        'prompt': prompt,\n",
    "    }\n",
    "    custom_examples.append(example)\n",
    "\n",
    "# Format examples using the chat template\n",
    "formatted_examples = []\n",
    "for i, example in enumerate(custom_examples):\n",
    "    formatted_example = format_with_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=base_model.tokenizer,\n",
    "        chat_template=gemma2_chat_template\n",
    "    )\n",
    "    formatted_examples.append(formatted_example)\n",
    "\n",
    "print(\"Custom prompts (formatted):\")\n",
    "for i, example in enumerate(formatted_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"PROMPT:\")\n",
    "    print(example['text_prompt'])\n",
    "    # Note: chosen/rejected will be empty for custom prompts\n",
    "    if example.get('text_chosen'):\n",
    "        print(\"\\nCHOSEN:\")\n",
    "        print(example['text_chosen'])\n",
    "    if example.get('text_rejected'):\n",
    "        print(\"\\nREJECTED:\")\n",
    "        print(example['text_rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a224c",
   "metadata": {},
   "source": [
    "## Generate Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "88d5e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating unsteered response for prompt 1...\n",
      "Generating steered response for prompt 1...\n",
      "Generating steered response for prompt 1...\n",
      "Processed custom prompt 1/1\n",
      "\n",
      "All outputs saved to: outputs/custom_prompts_comparison.txt\n",
      "Generated 1 steered outputs for analysis\n",
      "\n",
      "================================================================================\n",
      "RESULTS PREVIEW:\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Custom Prompt 1\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "\"I have 3 apples and give 3 away. What do I have now?\"\n",
      "\n",
      "FORMATTED PROMPT (for generation):\n",
      "<bos><start_of_turn>user\n",
      "I have 3 apples and give 3 away. What do I have now?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "UNSTEERED OUTPUT:\n",
      "user\n",
      "I have 3 apples and give 3 away. What do I have now?\n",
      "model\n",
      "You would have **zero** apples left. \n",
      "\n",
      "The classic math riddle.  Good one! 😄 \n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEERED OUTPUT:\n",
      "user\n",
      "I have 3 apples and give 3 away. What do I have now?\n",
      "model\n",
      "You have **0** apples left.\n",
      "\n",
      "Processed custom prompt 1/1\n",
      "\n",
      "All outputs saved to: outputs/custom_prompts_comparison.txt\n",
      "Generated 1 steered outputs for analysis\n",
      "\n",
      "================================================================================\n",
      "RESULTS PREVIEW:\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Custom Prompt 1\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "\"I have 3 apples and give 3 away. What do I have now?\"\n",
      "\n",
      "FORMATTED PROMPT (for generation):\n",
      "<bos><start_of_turn>user\n",
      "I have 3 apples and give 3 away. What do I have now?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "UNSTEERED OUTPUT:\n",
      "user\n",
      "I have 3 apples and give 3 away. What do I have now?\n",
      "model\n",
      "You would have **zero** apples left. \n",
      "\n",
      "The classic math riddle.  Good one! 😄 \n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEERED OUTPUT:\n",
      "user\n",
      "I have 3 apples and give 3 away. What do I have now?\n",
      "model\n",
      "You have **0** apples left.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 256\n",
    "\n",
    "steered_outputs = []\n",
    "output_buffer = []  # Buffer to collect all output text\n",
    "\n",
    "for i, example in enumerate(formatted_examples):\n",
    "    # Format the same example for generation (with generation prompt)\n",
    "    custom_example = custom_examples[i]  # Use custom_examples instead of shortest_examples\n",
    "    generation_example = format_for_generation(\n",
    "        example=custom_example,\n",
    "        tokenizer=base_model.tokenizer,\n",
    "        chat_template=gemma2_chat_template\n",
    "    )\n",
    "    generation_prompt = generation_example['text_prompt']\n",
    "    \n",
    "    buffer_text = f\"\\n{'='*80}\\n\"\n",
    "    buffer_text += f\"Custom Prompt {i+1}\\n\"\n",
    "    buffer_text += f\"{'='*80}\\n\\n\"\n",
    "    \n",
    "    buffer_text += \"ORIGINAL PROMPT:\\n\"\n",
    "    buffer_text += f'\"{custom_prompts[i]}\"\\n\\n'\n",
    "    \n",
    "    buffer_text += \"FORMATTED PROMPT (for generation):\\n\"\n",
    "    buffer_text += generation_prompt + \"\\n\"\n",
    "    \n",
    "    # Generate without steering\n",
    "    print(f\"Generating unsteered response for prompt {i+1}...\")\n",
    "    hooked_model.disable_steering()\n",
    "    unsteered_output = hooked_model.generate(generation_prompt, max_new_tokens=max_new_tokens, verbose=False)\n",
    "    buffer_text += \"\\n\" + \"-\"*60 + \"\\n\"\n",
    "    buffer_text += \"UNSTEERED OUTPUT:\\n\"\n",
    "    buffer_text += unsteered_output + \"\\n\"\n",
    "    \n",
    "    # Generate with steering\n",
    "    print(f\"Generating steered response for prompt {i+1}...\")\n",
    "    hooked_model.enable_steering()\n",
    "    steered_output = hooked_model.generate(generation_prompt, max_new_tokens=max_new_tokens, verbose=False)\n",
    "    buffer_text += \"\\n\" + \"-\"*60 + \"\\n\"\n",
    "    buffer_text += \"STEERED OUTPUT:\\n\"\n",
    "    buffer_text += steered_output + \"\\n\"\n",
    "\n",
    "    steered_outputs.append(steered_output)\n",
    "    output_buffer.append(buffer_text)\n",
    "    \n",
    "    print(f\"Processed custom prompt {i+1}/{len(formatted_examples)}\")\n",
    "\n",
    "# Write all outputs to a text file\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create outputs directory if it doesn't exist\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Simple filename that will be overwritten each run\n",
    "filename = \"outputs/custom_prompts_comparison.txt\"\n",
    "\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"FSRL Model Analysis - Custom Prompts Comparison\\n\")\n",
    "    f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Model: {selected_model}\\n\")\n",
    "    f.write(f\"Max new tokens: {max_new_tokens}\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\\n\")\n",
    "    \n",
    "    for buffer_text in output_buffer:\n",
    "        f.write(buffer_text)\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {filename}\")\n",
    "print(f\"Generated {len(steered_outputs)} steered outputs for analysis\")\n",
    "\n",
    "# Also display the results in the notebook\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS PREVIEW:\")\n",
    "print(\"=\"*80)\n",
    "for i, buffer_text in enumerate(output_buffer):\n",
    "    print(buffer_text)\n",
    "    if i < len(output_buffer) - 1:  # Don't add extra separator after last item\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3ea54e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_l0_norm(tensor):\n",
    "    \"\"\"\n",
    "    Calculate the average L0 norm of a tensor.\n",
    "    \n",
    "    L0 norm counts the number of non-zero elements.\n",
    "    For a tensor with multiple dimensions, we calculate the L0 norm\n",
    "    across the feature dimension and then average across other dimensions.\n",
    "    \n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape (batch, seq_len, features) or similar\n",
    "        \n",
    "    Returns:\n",
    "        float: Average L0 norm (average number of non-zero features)\n",
    "    \"\"\"\n",
    "    # Count non-zero elements across the last dimension (features)\n",
    "    l0_norms = torch.count_nonzero(tensor, dim=-1).float()\n",
    "    \n",
    "    # Average across all other dimensions\n",
    "    avg_l0_norm = l0_norms.mean().item()\n",
    "    \n",
    "    return avg_l0_norm\n",
    "\n",
    "def calculate_average_l1_norm(tensor):\n",
    "    \"\"\"\n",
    "    Calculate the average L1 norm of a tensor.\n",
    "    \n",
    "    L1 norm is the sum of absolute values of elements.\n",
    "    For a tensor with multiple dimensions, we calculate the L1 norm\n",
    "    across the feature dimension and then average across other dimensions.\n",
    "    \n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape (batch, seq_len, features) or similar\n",
    "        \n",
    "    Returns:\n",
    "        float: Average L1 norm (average sum of absolute values)\n",
    "    \"\"\"\n",
    "    # Calculate L1 norm across the last dimension (features)\n",
    "    l1_norms = torch.sum(torch.abs(tensor), dim=-1)\n",
    "    \n",
    "    # Average across all other dimensions\n",
    "    avg_l1_norm = l1_norms.mean().item()\n",
    "    \n",
    "    return avg_l1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0bb9ebc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing output 1/1...\n",
      "  Shape: torch.Size([1, 31, 65536])\n",
      "  L0 norm: 8639.87, L1 norm: 394.0000\n",
      "\n",
      "Processed 1 outputs\n",
      "  Shape: torch.Size([1, 31, 65536])\n",
      "  L0 norm: 8639.87, L1 norm: 394.0000\n",
      "\n",
      "Processed 1 outputs\n"
     ]
    }
   ],
   "source": [
    "# Analyze activations for each output individually\n",
    "individual_stats = []\n",
    "\n",
    "for i, output in enumerate(steered_outputs):\n",
    "    print(f\"Processing output {i+1}/{len(steered_outputs)}...\")\n",
    "    \n",
    "    # Get activations for this specific output\n",
    "    logits, cache = hooked_model.run_with_cache(output)\n",
    "    sae_activations = cache['blocks.12.hook_resid_post.hook_sae_adapter']\n",
    "    \n",
    "    # Calculate norms for this output\n",
    "    l0_norm = calculate_average_l0_norm(sae_activations)\n",
    "    l1_norm = calculate_average_l1_norm(sae_activations)\n",
    "    \n",
    "    # Store individual statistics\n",
    "    stats = {\n",
    "        'output_idx': i,\n",
    "        'shape': sae_activations.shape,\n",
    "        'l0_norm': l0_norm,\n",
    "        'l1_norm': l1_norm,\n",
    "        'l1_per_active_feature': l1_norm / l0_norm if l0_norm > 0 else 0,\n",
    "        'percent_active': (l0_norm / sae_activations.shape[-1]) * 100\n",
    "    }\n",
    "    individual_stats.append(stats)\n",
    "    \n",
    "    print(f\"  Shape: {sae_activations.shape}\")\n",
    "    print(f\"  L0 norm: {l0_norm:.2f}, L1 norm: {l1_norm:.4f}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(individual_stats)} outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6af92178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AGGREGATE STATISTICS ===\n",
      "Number of outputs analyzed: 1\n",
      "Total features per output: 65536\n",
      "\n",
      "L0 Norm (Sparsity) Statistics:\n",
      "  Mean: 8639.87\n",
      "  Std:  0.00\n",
      "  Min:  8639.87\n",
      "  Max:  8639.87\n",
      "\n",
      "L1 Norm Statistics:\n",
      "  Mean: 394.0000\n",
      "  Std:  0.0000\n",
      "  Min:  394.0000\n",
      "  Max:  394.0000\n",
      "\n",
      "L1 per Active Feature Statistics:\n",
      "  Mean: 0.0456\n",
      "  Std:  0.0000\n",
      "  Min:  0.0456\n",
      "  Max:  0.0456\n",
      "\n",
      "Percentage of Features Active:\n",
      "  Mean: 13.18%\n",
      "  Std:  0.00%\n",
      "  Min:  13.18%\n",
      "  Max:  13.18%\n",
      "\n",
      "=== INDIVIDUAL OUTPUT DETAILS ===\n",
      "Output 1: L0=8639.87, L1=394.0000, L1/L0=0.0456, Active=13.2%\n"
     ]
    }
   ],
   "source": [
    "# Compute aggregate statistics from individual outputs\n",
    "import numpy as np\n",
    "\n",
    "l0_norms = [stats['l0_norm'] for stats in individual_stats]\n",
    "l1_norms = [stats['l1_norm'] for stats in individual_stats]\n",
    "l1_per_active = [stats['l1_per_active_feature'] for stats in individual_stats]\n",
    "percent_active = [stats['percent_active'] for stats in individual_stats]\n",
    "\n",
    "print(\"=== AGGREGATE STATISTICS ===\")\n",
    "print(f\"Number of outputs analyzed: {len(individual_stats)}\")\n",
    "\n",
    "# Get feature count (should be same for all)\n",
    "total_features = individual_stats[0]['shape'][-1]\n",
    "print(f\"Total features per output: {total_features}\")\n",
    "\n",
    "print(f\"\\nL0 Norm (Sparsity) Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l0_norms):.2f}\")\n",
    "print(f\"  Std:  {np.std(l0_norms):.2f}\")\n",
    "print(f\"  Min:  {np.min(l0_norms):.2f}\")\n",
    "print(f\"  Max:  {np.max(l0_norms):.2f}\")\n",
    "\n",
    "print(f\"\\nL1 Norm Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l1_norms):.4f}\")\n",
    "print(f\"  Std:  {np.std(l1_norms):.4f}\")\n",
    "print(f\"  Min:  {np.min(l1_norms):.4f}\")\n",
    "print(f\"  Max:  {np.max(l1_norms):.4f}\")\n",
    "\n",
    "print(f\"\\nL1 per Active Feature Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l1_per_active):.4f}\")\n",
    "print(f\"  Std:  {np.std(l1_per_active):.4f}\")\n",
    "print(f\"  Min:  {np.min(l1_per_active):.4f}\")\n",
    "print(f\"  Max:  {np.max(l1_per_active):.4f}\")\n",
    "\n",
    "print(f\"\\nPercentage of Features Active:\")\n",
    "print(f\"  Mean: {np.mean(percent_active):.2f}%\")\n",
    "print(f\"  Std:  {np.std(percent_active):.2f}%\")\n",
    "print(f\"  Min:  {np.min(percent_active):.2f}%\")\n",
    "print(f\"  Max:  {np.max(percent_active):.2f}%\")\n",
    "\n",
    "print(f\"\\n=== INDIVIDUAL OUTPUT DETAILS ===\")\n",
    "for i, stats in enumerate(individual_stats):\n",
    "    print(f\"Output {i+1}: L0={stats['l0_norm']:.2f}, L1={stats['l1_norm']:.4f}, \"\n",
    "          f\"L1/L0={stats['l1_per_active_feature']:.4f}, Active={stats['percent_active']:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
