{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0034bdb",
   "metadata": {},
   "source": [
    "# FSRL Model Analysis\n",
    "\n",
    "This notebook analyzes trained Feature Steering RL models by comparing steered vs unsteered generation outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cea6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from fsrl import SAEAdapter, HookedModel\n",
    "from fsrl.utils.wandb_utils import WandBModelDownloader\n",
    "from fsrl.simPO.utils import apply_chat_template\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbf0e6",
   "metadata": {},
   "source": [
    "## Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891364af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact adapter-light-yogurt-6:v0, 288.25MB. 2 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
      "Done. 0:0:0.4 (790.1MB/s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact adapter-giddy-silence-7:v0, 288.25MB. 2 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
      "Done. 0:0:7.0 (41.1MB/s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['giddy-silence-7', 'light-yogurt-6', 'rosy-cloud-4']\n"
     ]
    }
   ],
   "source": [
    "project_name = \"Gemma2-2B-train-ablate\"\n",
    "downloader = WandBModelDownloader(entity=\"feature-steering-RL\", project=project_name)\n",
    "downloader.download_all_models(project_name)\n",
    "\n",
    "available_models = downloader.list_downloaded_models(project_name)\n",
    "print(f\"Available models: {available_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d3e940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: light-yogurt-6\n"
     ]
    }
   ],
   "source": [
    "selected_model = 'light-yogurt-6'\n",
    "print(f\"Using: {selected_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321fb69f",
   "metadata": {},
   "source": [
    "## Load Model and Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695ad12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3cbcaf120e4b04ab8e905c8a3031b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b-it into HookedTransformer\n",
      "Loading adapter from: /lambda/nfs/fsrl/feature-steering-RL/models/Gemma2-2B-train-ablate/light-yogurt-6/adapter\n",
      "Adapter loaded from /lambda/nfs/fsrl/feature-steering-RL/models/Gemma2-2B-train-ablate/light-yogurt-6/adapter\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "base_model = HookedTransformer.from_pretrained_no_processing(\"google/gemma-2-2b-it\", device=\"cuda\", dtype=torch.bfloat16)\n",
    "adapter_path = downloader.models_base_dir / project_name / selected_model / \"adapter\"\n",
    "print(f\"Loading adapter from: {adapter_path}\")\n",
    "\n",
    "sae_adapter = SAEAdapter.load_from_pretrained_adapter(adapter_path, device=\"cuda\")\n",
    "hooked_model = HookedModel(base_model, sae_adapter)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0042e7",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867ed752",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"princeton-nlp/llama3-ultrafeedback-armorm\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91bd41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "num_samples = 5\n",
    "\n",
    "# Get 5 examples with shortest prompts\n",
    "df = pd.DataFrame(dataset)\n",
    "df['prompt_length'] = df['prompt'].str.len()\n",
    "shortest_examples = df.nsmallest(num_samples, 'prompt_length')[['prompt', 'chosen', 'rejected']]\n",
    "\n",
    "# Define the Gemma2 chat template (from config)\n",
    "gemma2_chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] | trim + '\\n\\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + content | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "\n",
    "def format_with_chat_template(example, tokenizer, chat_template):\n",
    "    \"\"\"Format an example using the chat template.\"\"\"\n",
    "    # Set the chat template\n",
    "    tokenizer.chat_template = chat_template\n",
    "    \n",
    "    # Apply chat template to get properly formatted text\n",
    "    formatted_example = apply_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=tokenizer,\n",
    "        chat_template=chat_template,\n",
    "        task=\"simpo\",\n",
    "        auto_insert_empty_system_msg=True\n",
    "    )\n",
    "    \n",
    "    return formatted_example\n",
    "\n",
    "def format_for_generation(example, tokenizer, chat_template):\n",
    "    \"\"\"Format an example for generation with generation prompt.\"\"\"\n",
    "    # Set the chat template\n",
    "    tokenizer.chat_template = chat_template\n",
    "    \n",
    "    # Apply chat template to get properly formatted text with generation prompt\n",
    "    formatted_example = apply_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=tokenizer,\n",
    "        chat_template=chat_template,\n",
    "        task=\"simpo_generation\",\n",
    "        auto_insert_empty_system_msg=True\n",
    "    )\n",
    "    \n",
    "    return formatted_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec93406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom prompts (formatted):\n",
      "\n",
      "Example 1:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "I have 3 apples and give 3 away. What do I have now?<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "Example 2:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "What is the meaning of life?<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "Example 3:\n",
      "PROMPT:\n",
      "<bos><start_of_turn>user\n",
      "Explain the theory of relativity in simple terms.<end_of_turn>\n",
      "\n",
      "\n",
      "CHOSEN:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n",
      "\n",
      "REJECTED:\n",
      "<start_of_turn>model\n",
      "<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create custom examples to test\n",
    "custom_prompts = [\n",
    "    \"I have 3 apples and give 3 away. What do I have now?\",\n",
    "    \"What is the meaning of life?\",\n",
    "    \"Explain the theory of relativity in simple terms.\",\n",
    "]\n",
    "\n",
    "# Convert custom prompts to proper OpenAI format expected by simpo task\n",
    "custom_examples = []\n",
    "for prompt in custom_prompts:\n",
    "    # For simpo task, we need chosen and rejected as conversation histories\n",
    "    # Since we don't have real chosen/rejected responses, we'll create minimal examples\n",
    "    example = {\n",
    "        'chosen': [\n",
    "            {'role': 'user', 'content': prompt},\n",
    "            {'role': 'assistant', 'content': ''}  # Empty response - we'll generate this\n",
    "        ],\n",
    "        'rejected': [\n",
    "            {'role': 'user', 'content': prompt},\n",
    "            {'role': 'assistant', 'content': ''}  # Empty response - we'll generate this  \n",
    "        ],\n",
    "        'prompt': prompt,\n",
    "    }\n",
    "    custom_examples.append(example)\n",
    "\n",
    "# Format examples using the chat template\n",
    "formatted_examples = []\n",
    "for i, example in enumerate(custom_examples):\n",
    "    formatted_example = format_with_chat_template(\n",
    "        example=example,\n",
    "        tokenizer=base_model.tokenizer,\n",
    "        chat_template=gemma2_chat_template\n",
    "    )\n",
    "    formatted_examples.append(formatted_example)\n",
    "\n",
    "print(\"Custom prompts (formatted):\")\n",
    "for i, example in enumerate(formatted_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"PROMPT:\")\n",
    "    print(example['text_prompt'])\n",
    "    # Note: chosen/rejected will be empty for custom prompts\n",
    "    if example.get('text_chosen'):\n",
    "        print(\"\\nCHOSEN:\")\n",
    "        print(example['text_chosen'])\n",
    "    if example.get('text_rejected'):\n",
    "        print(\"\\nREJECTED:\")\n",
    "        print(example['text_rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a224c",
   "metadata": {},
   "source": [
    "## Generate Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88d5e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating unsteered response for prompt 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating steered response for prompt 1...\n",
      "Processed custom prompt 1/3\n",
      "Generating unsteered response for prompt 2...\n",
      "Generating steered response for prompt 2...\n",
      "Processed custom prompt 2/3\n",
      "Generating unsteered response for prompt 3...\n",
      "Generating steered response for prompt 3...\n",
      "Processed custom prompt 3/3\n",
      "\n",
      "All outputs saved to: outputs/custom_prompts_comparison.txt\n",
      "Generated 3 steered outputs for analysis\n",
      "\n",
      "================================================================================\n",
      "RESULTS PREVIEW:\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Custom Prompt 1\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "\"I have 3 apples and give 3 away. What do I have now?\"\n",
      "\n",
      "FORMATTED PROMPT (for generation):\n",
      "<bos><start_of_turn>user\n",
      "I have 3 apples and give 3 away. What do I have now?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "UNSTEERED OUTPUT:\n",
      "user\n",
      "I have 3 apples and give 3 away. What do I have now?\n",
      "model\n",
      "You would have **0** apples. \n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEERED OUTPUT:\n",
      "user\n",
      "I have 3 apples and give 3 away. What do I have now?\n",
      "model\n",
      "You‚Äôll**beholdingrif**:1remaining apple \n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Custom Prompt 2\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "\"What is the meaning of life?\"\n",
      "\n",
      "FORMATTED PROMPT (for generation):\n",
      "<bos><start_of_turn>user\n",
      "What is the meaning of life?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "UNSTEERED OUTPUT:\n",
      "user\n",
      "What is the meaning of life?\n",
      "model\n",
      "The meaning of life is a question that philosophers, theologians, and individuals have grappled with for millennia. There‚Äôs no single, universally accepted answer, and what resonates with one person may not resonate with another.\n",
      "\n",
      "Here are some perspectives: \n",
      "\n",
      "**Philosophical Perspectives:**\n",
      "\n",
      "* **Nihilism:** There is no inherent meaning, purpose, or value to life. Life is meaningless by nature.\n",
      "* **Existentialism:** Individuals create their own meaning through their actions and choices. Existence precedes essence. \n",
      "* **Absurdism:** The human attempt to find meaning in a meaningless universe.\n",
      "* **Humanism:**  Meaning is found in human experiences, relationships, and the pursuit of reason and ethics.\n",
      "* **Hedonism:**  The pursuit of pleasure and happiness is the ultimate goal of life.\n",
      "* **Stoicism:**  Living a virtuous life in accordance with nature, finding meaning in what is within your control (like your actions and thoughts).\n",
      "* **Utilitarianism:**  Maximizing happiness and well-being for the greatest number of people is the best way to achieve meaning.\n",
      "\n",
      "**Spiritual/Religious Perspectives:**\n",
      "\n",
      "* **Religion:** Many religions offer answers based on their specific beliefs, such as finding meaning through worship\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEERED OUTPUT:\n",
      "user\n",
      "What is the meaning of life?\n",
      "model\n",
      "That‚Äôsa_rather_population-sizedquestionwithaverypersonalandthought-provokingissueinforeveryblokinneritivitysourceÔºÅNooneexplicitlydelivers‚Äòthesolution.‚ÄôIt‚Äôsverymuchasucceedingwithyourinterpretation\"\"\"beginnings‚ãÖmeaning\\[‚Äò„ÅÑ„ÅÆ„Å°„ÅÆÊú¨Ë≥™‚Äô,‚ÄòÊÑèÁæ©‚Äô]**isshapedbyYourinnerjobdescription:\n",
      "*cernsettingyourheart[‰æãÔºö‰∫∫Áîü„Å£„Å¶‚ÄòÔºàÊÜß„Çå„ÅÆÔºâ\\u30A1\\u308w(Â±Ö=‚ÄôÂÄã‰Ωì„Å®„Åó„Å¶„ÅÆËºù„Åç*ÂÄã„ÄÖ„ÇíË≤´„ÅçÈÄö„ÅôÊóÖÁä∂Áä∂Áä∂Œ†Œ±œÅŒ±œÄŒøŒºœÄŒ≠œÇschUObjectconstructor( ya≈ülanƒ±ncaÔºü)\n",
      "*dimensionÔºü‚Äùasyoureceivedicts\n",
      "    *‚ÄòÂäüÁ∏æ‚Äôtolawof_\"+yourstrugglingnatureksiyonu„ÉªÊÄùÊÉ≥\\{\\‚ÄôÊÑèÁæ©ÊâÄÂú®‚Äô„ÅØ„ÄÅË¶ñÁÇπ„Å´„Çà„Çä```\n",
      "***<‰∏â„Å§„ÅÆÂàá„ÇäÂàÜ„ÅëÊñπÊ≥ï„Éª„ÅÇ„Å™„Åü„ÅÆÂÄãË£Ö„ÇíÊõ∏„ÅçWhenactin‚Äô\"}üêæ tipiƒ±othingever?\n",
      "1.ExistenceitselfÔºö**„ÅÑ„ÅÆ„Å°„ÅÆÂëΩ‰ª§/_„ÇíËá™ÂàÜ„ÅßÂèó„Åë„Å®„ÇÅ„Çã**shall\"ers√ºreÔºà„ÅÑ„Å°„ÅØ„Åò„Åã„ÅÑ judg‚ÄôsÔºâ+:160(‰æã:Èñ¢„Çè„Çã„Åì„Å®„ÄÅÁêÜËß£)\n",
      "Ë°å flamme89Ê≠≥ octogenerian1940Âπ¥‰ª£Áîü„Åæ„Çå„ÅÆ\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Custom Prompt 3\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "\"Explain the theory of relativity in simple terms.\"\n",
      "\n",
      "FORMATTED PROMPT (for generation):\n",
      "<bos><start_of_turn>user\n",
      "Explain the theory of relativity in simple terms.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "UNSTEERED OUTPUT:\n",
      "user\n",
      "Explain the theory of relativity in simple terms.\n",
      "model\n",
      "Imagine you're throwing a ball in a car that's traveling at a very fast speed. From your perspective on the ground, the ball travels in a straight line. But to someone watching from a train moving at the same speed as the car, the ball looks like it's traveling in a diagonal path!\n",
      "\n",
      "Here's the gist:\n",
      "\n",
      "**Einstein's theory of relativity tells us that:**\n",
      "\n",
      "* **Motion is relative:**  The way we perceive motion depends on our reference point. This means there's no absolute \"ground truth\". \n",
      "* **Space and time are intertwined:** They are not independent entities, but form a four-dimensional fabric called spacetime. Things don't exist in separate lanes, they exist in a connected system.\n",
      "\n",
      "**There are two components of relativity:**\n",
      "\n",
      "**1. Special Relativity:** Deals with objects moving at incredibly high speeds, where things get weird. \n",
      "\n",
      "* **The speed of light is constant:** No matter how fast you're moving (relative to others), the speed of light always appears the same (about 299,792,458 meters per second). \n",
      "* **Time dilation:** Moving clocks tick slower relative to stationary ones. The faster you move,\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEERED OUTPUT:\n",
      "user\n",
      "Explain the theory of relativity in simple terms.\n",
      "model\n",
      "Insimpleterms,\\(RLoÔºåShow usyourbestversion1.50**digits.2alternateshersomehowin Carla‚Äôsdropdownboxranninpacespaceflight](‚ÄªÔºâÔºàÊïÖ‰∫ãÊòØÊàëËá™Â∑±ÁöÑÔºåËØ∑ÂãøÊäÑË¢≠Ôºâ\n",
      "Ê¥õ‰º¶ÁâπlectricicfirerevolutionizedjourneWithDataSynthesisModel:**Einstein‚Äôsthinkingwentlikethis:\\\\n\\\n",
      "**CoreConcepts**:\n",
      "* **ConstantsGonnaHijackersExplanationConventionÔºà‚ÄòelativeÔºâ:**ClassicsÊòØ(.*),wekum?‚Äùexecution‰ª£Á†ÅÊòØ‰∏∫‰∫∫Á±ªËÆæËÆ°ÔºåÂπ∂ÈùûÊäÄÊúØÈÅµ‰ªéÔºârepresents___iendoantionwith,motion_concerningphysicsandfundamentalsof**indiludnessnumberthingnessÔºànotjust‚Äúsomethingthatmoves‚ÄùÔºâ\\(**Êï∞Â≠¶Á¨¶Âè∑Ôºötipoconverts\n",
      "\\[‚Äù<\\utiliteral‡§ö‡§≤‡§æ‡§á‡§§‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ÔºàdebugËâ∫ÊúØÈ¢òÔºüÔºâÔºâ\\}{\\verb>\\bold{[invariance]}}\\__ preuve\\textit{isbasedon](executionnow commencementofinitialization}\")\n",
      "\\[\\text{Youruserismoderntypelocationprioritiesformance}\n",
      "\\begin{enumerate}\\listing{defaultpage=31}\n",
      "approach.\n",
      "*atime(imageduringthat‚ÄúRepublic(Refined)events(`special‚Äô\"},info='_local‚Äô,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 256\n",
    "\n",
    "steered_outputs = []\n",
    "output_buffer = []  # Buffer to collect all output text\n",
    "\n",
    "for i, example in enumerate(formatted_examples):\n",
    "    # Format the same example for generation (with generation prompt)\n",
    "    custom_example = custom_examples[i]  # Use custom_examples instead of shortest_examples\n",
    "    generation_example = format_for_generation(\n",
    "        example=custom_example,\n",
    "        tokenizer=base_model.tokenizer,\n",
    "        chat_template=gemma2_chat_template\n",
    "    )\n",
    "    generation_prompt = generation_example['text_prompt']\n",
    "    \n",
    "    buffer_text = f\"\\n{'='*80}\\n\"\n",
    "    buffer_text += f\"Custom Prompt {i+1}\\n\"\n",
    "    buffer_text += f\"{'='*80}\\n\\n\"\n",
    "    \n",
    "    buffer_text += \"ORIGINAL PROMPT:\\n\"\n",
    "    buffer_text += f'\"{custom_prompts[i]}\"\\n\\n'\n",
    "    \n",
    "    buffer_text += \"FORMATTED PROMPT (for generation):\\n\"\n",
    "    buffer_text += generation_prompt + \"\\n\"\n",
    "    \n",
    "    # Generate without steering\n",
    "    print(f\"Generating unsteered response for prompt {i+1}...\")\n",
    "    hooked_model.disable_steering()\n",
    "    unsteered_output = hooked_model.generate(generation_prompt, max_new_tokens=max_new_tokens, verbose=False)\n",
    "    buffer_text += \"\\n\" + \"-\"*60 + \"\\n\"\n",
    "    buffer_text += \"UNSTEERED OUTPUT:\\n\"\n",
    "    buffer_text += unsteered_output + \"\\n\"\n",
    "    \n",
    "    # Generate with steering\n",
    "    print(f\"Generating steered response for prompt {i+1}...\")\n",
    "    hooked_model.enable_steering()\n",
    "    steered_output = hooked_model.generate(generation_prompt, max_new_tokens=max_new_tokens, verbose=False)\n",
    "    buffer_text += \"\\n\" + \"-\"*60 + \"\\n\"\n",
    "    buffer_text += \"STEERED OUTPUT:\\n\"\n",
    "    buffer_text += steered_output + \"\\n\"\n",
    "\n",
    "    steered_outputs.append(steered_output)\n",
    "    output_buffer.append(buffer_text)\n",
    "    \n",
    "    print(f\"Processed custom prompt {i+1}/{len(formatted_examples)}\")\n",
    "\n",
    "# Write all outputs to a text file\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create outputs directory if it doesn't exist\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Simple filename that will be overwritten each run\n",
    "filename = \"outputs/custom_prompts_comparison.txt\"\n",
    "\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"FSRL Model Analysis - Custom Prompts Comparison\\n\")\n",
    "    f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Model: {selected_model}\\n\")\n",
    "    f.write(f\"Max new tokens: {max_new_tokens}\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\\n\")\n",
    "    \n",
    "    for buffer_text in output_buffer:\n",
    "        f.write(buffer_text)\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {filename}\")\n",
    "print(f\"Generated {len(steered_outputs)} steered outputs for analysis\")\n",
    "\n",
    "# Also display the results in the notebook\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS PREVIEW:\")\n",
    "print(\"=\"*80)\n",
    "for i, buffer_text in enumerate(output_buffer):\n",
    "    print(buffer_text)\n",
    "    if i < len(output_buffer) - 1:  # Don't add extra separator after last item\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea54e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_l0_norm(tensor):\n",
    "    \"\"\"\n",
    "    Calculate the average L0 norm of a tensor.\n",
    "    \n",
    "    L0 norm counts the number of non-zero elements.\n",
    "    For a tensor with multiple dimensions, we calculate the L0 norm\n",
    "    across the feature dimension and then average across other dimensions.\n",
    "    \n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape (batch, seq_len, features) or similar\n",
    "        \n",
    "    Returns:\n",
    "        float: Average L0 norm (average number of non-zero features)\n",
    "    \"\"\"\n",
    "    # Count non-zero elements across the last dimension (features)\n",
    "    l0_norms = torch.count_nonzero(tensor, dim=-1).float()\n",
    "    \n",
    "    # Average across all other dimensions\n",
    "    avg_l0_norm = l0_norms.mean().item()\n",
    "    \n",
    "    return avg_l0_norm\n",
    "\n",
    "def calculate_average_l1_norm(tensor):\n",
    "    \"\"\"\n",
    "    Calculate the average L1 norm of a tensor.\n",
    "    \n",
    "    L1 norm is the sum of absolute values of elements.\n",
    "    For a tensor with multiple dimensions, we calculate the L1 norm\n",
    "    across the feature dimension and then average across other dimensions.\n",
    "    \n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape (batch, seq_len, features) or similar\n",
    "        \n",
    "    Returns:\n",
    "        float: Average L1 norm (average sum of absolute values)\n",
    "    \"\"\"\n",
    "    # Calculate L1 norm across the last dimension (features)\n",
    "    l1_norms = torch.sum(torch.abs(tensor), dim=-1)\n",
    "    \n",
    "    # Average across all other dimensions\n",
    "    avg_l1_norm = l1_norms.mean().item()\n",
    "    \n",
    "    return avg_l1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb9ebc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing output 1/3...\n",
      "  Shape: torch.Size([1, 36, 65536])\n",
      "  L0 norm: 11271.25, L1 norm: 160.0000\n",
      "Processing output 2/3...\n",
      "  Shape: torch.Size([1, 267, 65536])\n",
      "  L0 norm: 11106.63, L1 norm: 25.3750\n",
      "Processing output 3/3...\n",
      "  Shape: torch.Size([1, 275, 65536])\n",
      "  L0 norm: 11126.19, L1 norm: 26.5000\n",
      "\n",
      "Processed 3 outputs\n"
     ]
    }
   ],
   "source": [
    "# Analyze activations for each output individually\n",
    "individual_stats = []\n",
    "\n",
    "for i, output in enumerate(steered_outputs):\n",
    "    print(f\"Processing output {i+1}/{len(steered_outputs)}...\")\n",
    "    \n",
    "    # Get activations for this specific output\n",
    "    logits, cache = hooked_model.run_with_cache(output)\n",
    "    sae_activations = cache['blocks.12.hook_resid_post.hook_sae_adapter']\n",
    "    \n",
    "    # Calculate norms for this output\n",
    "    l0_norm = calculate_average_l0_norm(sae_activations)\n",
    "    l1_norm = calculate_average_l1_norm(sae_activations)\n",
    "    \n",
    "    # Store individual statistics\n",
    "    stats = {\n",
    "        'output_idx': i,\n",
    "        'shape': sae_activations.shape,\n",
    "        'l0_norm': l0_norm,\n",
    "        'l1_norm': l1_norm,\n",
    "        'l1_per_active_feature': l1_norm / l0_norm if l0_norm > 0 else 0,\n",
    "        'percent_active': (l0_norm / sae_activations.shape[-1]) * 100\n",
    "    }\n",
    "    individual_stats.append(stats)\n",
    "    \n",
    "    print(f\"  Shape: {sae_activations.shape}\")\n",
    "    print(f\"  L0 norm: {l0_norm:.2f}, L1 norm: {l1_norm:.4f}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(individual_stats)} outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af92178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AGGREGATE STATISTICS ===\n",
      "Number of outputs analyzed: 3\n",
      "Total features per output: 65536\n",
      "\n",
      "L0 Norm (Sparsity) Statistics:\n",
      "  Mean: 11168.02\n",
      "  Std:  73.43\n",
      "  Min:  11106.63\n",
      "  Max:  11271.25\n",
      "\n",
      "L1 Norm Statistics:\n",
      "  Mean: 70.6250\n",
      "  Std:  63.1993\n",
      "  Min:  25.3750\n",
      "  Max:  160.0000\n",
      "\n",
      "L1 per Active Feature Statistics:\n",
      "  Mean: 0.0063\n",
      "  Std:  0.0056\n",
      "  Min:  0.0023\n",
      "  Max:  0.0142\n",
      "\n",
      "Percentage of Features Active:\n",
      "  Mean: 17.04%\n",
      "  Std:  0.11%\n",
      "  Min:  16.95%\n",
      "  Max:  17.20%\n",
      "\n",
      "=== INDIVIDUAL OUTPUT DETAILS ===\n",
      "Output 1: L0=11271.25, L1=160.0000, L1/L0=0.0142, Active=17.2%\n",
      "Output 2: L0=11106.63, L1=25.3750, L1/L0=0.0023, Active=16.9%\n",
      "Output 3: L0=11126.19, L1=26.5000, L1/L0=0.0024, Active=17.0%\n"
     ]
    }
   ],
   "source": [
    "# Compute aggregate statistics from individual outputs\n",
    "import numpy as np\n",
    "\n",
    "l0_norms = [stats['l0_norm'] for stats in individual_stats]\n",
    "l1_norms = [stats['l1_norm'] for stats in individual_stats]\n",
    "l1_per_active = [stats['l1_per_active_feature'] for stats in individual_stats]\n",
    "percent_active = [stats['percent_active'] for stats in individual_stats]\n",
    "\n",
    "print(\"=== AGGREGATE STATISTICS ===\")\n",
    "print(f\"Number of outputs analyzed: {len(individual_stats)}\")\n",
    "\n",
    "# Get feature count (should be same for all)\n",
    "total_features = individual_stats[0]['shape'][-1]\n",
    "print(f\"Total features per output: {total_features}\")\n",
    "\n",
    "print(f\"\\nL0 Norm (Sparsity) Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l0_norms):.2f}\")\n",
    "print(f\"  Std:  {np.std(l0_norms):.2f}\")\n",
    "print(f\"  Min:  {np.min(l0_norms):.2f}\")\n",
    "print(f\"  Max:  {np.max(l0_norms):.2f}\")\n",
    "\n",
    "print(f\"\\nL1 Norm Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l1_norms):.4f}\")\n",
    "print(f\"  Std:  {np.std(l1_norms):.4f}\")\n",
    "print(f\"  Min:  {np.min(l1_norms):.4f}\")\n",
    "print(f\"  Max:  {np.max(l1_norms):.4f}\")\n",
    "\n",
    "print(f\"\\nL1 per Active Feature Statistics:\")\n",
    "print(f\"  Mean: {np.mean(l1_per_active):.4f}\")\n",
    "print(f\"  Std:  {np.std(l1_per_active):.4f}\")\n",
    "print(f\"  Min:  {np.min(l1_per_active):.4f}\")\n",
    "print(f\"  Max:  {np.max(l1_per_active):.4f}\")\n",
    "\n",
    "print(f\"\\nPercentage of Features Active:\")\n",
    "print(f\"  Mean: {np.mean(percent_active):.2f}%\")\n",
    "print(f\"  Std:  {np.std(percent_active):.2f}%\")\n",
    "print(f\"  Min:  {np.min(percent_active):.2f}%\")\n",
    "print(f\"  Max:  {np.max(percent_active):.2f}%\")\n",
    "\n",
    "print(f\"\\n=== INDIVIDUAL OUTPUT DETAILS ===\")\n",
    "for i, stats in enumerate(individual_stats):\n",
    "    print(f\"Output {i+1}: L0={stats['l0_norm']:.2f}, L1={stats['l1_norm']:.4f}, \"\n",
    "          f\"L1/L0={stats['l1_per_active_feature']:.4f}, Active={stats['percent_active']:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
