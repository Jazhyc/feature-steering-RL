{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c14438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath(\"../../src\"))\n",
    "\n",
    "from fsrl import SAEAdapter, HookedModel\n",
    "from fsrl.utils.wandb_utils import WandBModelDownloader\n",
    "\n",
    "# Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "INPUT_FILE = \"example_outputs_input.json\"\n",
    "\n",
    "# Specify which WandB runs to analyze\n",
    "FSRL_PROJECT = \"Gemma2-2B-muon\"  # WandB project for FSRL model\n",
    "FSRL_RUN_NAME = \"mild-resonance-1\"  # Regular FSRL model run name\n",
    "\n",
    "FSRL_ABLATED_PROJECT = \"Gemma2-2B-train-ablate\"  # WandB project for ablated model\n",
    "FSRL_ABLATED_RUN_NAME = \"rosy-cloud-4\"  # Style ablated model run name\n",
    "\n",
    "# Feature Descriptions\n",
    "DESCRIPTIONS_FILE = \"../../models/NeuronpediaCache/gemma-2-2b/12-gemmascope-res-65k_canonical.json\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd33d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Feature Descriptions\n",
    "def load_descriptions(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: Descriptions file not found at {file_path}\")\n",
    "        return {}\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    descriptions = {}\n",
    "    \n",
    "    # Handle different formats\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            idx = item.get('index', item.get('feature_index'))\n",
    "            desc = item.get('description', item.get('explanation'))\n",
    "            if idx is not None:\n",
    "                descriptions[int(idx)] = desc\n",
    "    elif isinstance(data, dict):\n",
    "        for k, v in data.items():\n",
    "            try:\n",
    "                idx = int(k)\n",
    "                desc = v.get('description', v.get('explanation')) if isinstance(v, dict) else v\n",
    "                descriptions[idx] = desc\n",
    "            except ValueError:\n",
    "                pass\n",
    "                \n",
    "    print(f\"Loaded {len(descriptions)} descriptions.\")\n",
    "    return descriptions\n",
    "\n",
    "feature_descriptions = load_descriptions(DESCRIPTIONS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load a model with adapter\n",
    "def load_model_with_adapter(project, run_name, model_label):\n",
    "    downloader = WandBModelDownloader(\n",
    "        entity=\"feature-steering-RL\",\n",
    "        project=project,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    adapter_path = Path(\"../../models\") / project / run_name / \"adapter\"\n",
    "    \n",
    "    if not adapter_path.exists():\n",
    "        print(f\"Adapter not found at {adapter_path}\")\n",
    "        print(f\"Available runs in {project}:\", downloader.list_downloaded_models(project))\n",
    "        raise FileNotFoundError(f\"Please download the model first or change {model_label} settings\")\n",
    "    \n",
    "    print(f\"Loading {model_label}...\")\n",
    "    base_model = HookedTransformer.from_pretrained(\n",
    "        \"google/gemma-2-2b-it\",\n",
    "        device=DEVICE,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    print(f\"  Loading adapter from {adapter_path}\")\n",
    "    sae_adapter = SAEAdapter.load_from_pretrained_adapter(str(adapter_path), device=DEVICE)\n",
    "    \n",
    "    model = HookedModel(base_model, sae_adapter)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"  {model_label} loaded successfully!\")\n",
    "    return model\n",
    "\n",
    "# Load both models from their respective projects\n",
    "fsrl_model = load_model_with_adapter(FSRL_PROJECT, FSRL_RUN_NAME, \"FSRL Model\")\n",
    "fsrl_ablated_model = load_model_with_adapter(FSRL_ABLATED_PROJECT, FSRL_ABLATED_RUN_NAME, \"FSRL Ablated Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Function\n",
    "def analyze_steering(model, text, top_k=10):\n",
    "    \"\"\"Analyze which features are most activated for a given text.\"\"\"\n",
    "    # Tokenize\n",
    "    tokens = model.model.to_tokens(text)\n",
    "    \n",
    "    # Get the hook name from the adapter\n",
    "    layer = model.sae_adapter.cfg.hook_layer\n",
    "    hook_name = f\"blocks.{layer}.hook_resid_post.hook_sae_adapter\"\n",
    "    \n",
    "    # Run with cache\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "    if hook_name not in cache:\n",
    "        print(f\"Warning: Hook {hook_name} not found in cache.\")\n",
    "        print(f\"Available keys: {list(cache.keys())[:5]}...\")\n",
    "        return []\n",
    "        \n",
    "    # Get activations: [batch, seq_len, d_sae]\n",
    "    acts = cache[hook_name][0]  # Remove batch dim\n",
    "    \n",
    "    # Average across all tokens\n",
    "    mean_acts = acts.mean(dim=0)\n",
    "    \n",
    "    # Get top K features\n",
    "    top_values, top_indices = torch.topk(mean_acts, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for val, idx in zip(top_values, top_indices):\n",
    "        idx_val = idx.item()\n",
    "        results.append({\n",
    "            \"feature_index\": idx_val,\n",
    "            \"activation\": val.item(),\n",
    "            \"description\": feature_descriptions.get(idx_val, \"No description found\")\n",
    "        })\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de271f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze examples from JSON file\n",
    "with open(INPUT_FILE, 'r') as f:\n",
    "    examples = json.load(f)\n",
    "\n",
    "for i, ex in enumerate(examples):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"Prompt:\\n{ex['prompt']}\\n\")\n",
    "    print(f\"Baseline Output:\\n{ex['baseline_output']}\\n\")\n",
    "    \n",
    "    # FSRL Model Analysis\n",
    "    print(f\"--- FSRL Output ---\")\n",
    "    print(f\"{ex['fsrl_output']}\\n\")\n",
    "    \n",
    "    full_text_fsrl = ex['prompt'] + ex['fsrl_output']\n",
    "    fsrl_features = analyze_steering(fsrl_model, full_text_fsrl)\n",
    "    \n",
    "    print(\"Top Steered Features (FSRL):\")\n",
    "    for f in fsrl_features:\n",
    "        print(f\"  Feature {f['feature_index']}: {f['activation']:.4f}\")\n",
    "        print(f\"    {f['description']}\\n\")\n",
    "    \n",
    "    # FSRL Ablated Model Analysis\n",
    "    print(f\"--- FSRL Ablated Output ---\")\n",
    "    print(f\"{ex['fsrl_ablated_output']}\\n\")\n",
    "    \n",
    "    full_text_ablated = ex['prompt'] + ex['fsrl_ablated_output']\n",
    "    ablated_features = analyze_steering(fsrl_ablated_model, full_text_ablated)\n",
    "    \n",
    "    print(\"Top Steered Features (FSRL Ablated):\")\n",
    "    for f in ablated_features:\n",
    "        print(f\"  Feature {f['feature_index']}: {f['activation']:.4f}\")\n",
    "        print(f\"    {f['description']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
