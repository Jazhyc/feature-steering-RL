{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46721f3d",
   "metadata": {},
   "source": [
    "# Error Correction Analysis for Classification Noise\n",
    "\n",
    "This notebook performs error correction to account for classification noise in alignment and formatting feature labels. It implements the approach of solving a system of linear equations to estimate the \"pure\" loss per feature (LPF) for each category.\n",
    "\n",
    "## Approach:\n",
    "1. Load model classifications (DeepSeek) and human labels\n",
    "2. Compute confusion matrices to estimate contamination rates\n",
    "3. Solve system of equations to get corrected LPF values\n",
    "4. Compare naive vs corrected results\n",
    "\n",
    "## Data:\n",
    "- **Models**: Gemma 2 2B and Gemma 2 9B\n",
    "- **Categories**: Alignment and Formatting (Style)\n",
    "- **Losses**: Hardcoded from experimental results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d300bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Helper function to load JSON records\n",
    "def load_json_records(p: Path) -> list[dict]:\n",
    "    \"\"\"Load JSON data, handling both list and dict formats.\"\"\"\n",
    "    with p.open('r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    if isinstance(data, dict):\n",
    "        for k in (\"items\", \"records\", \"data\"):\n",
    "            if k in data and isinstance(data[k], list):\n",
    "                return data[k]\n",
    "        return [dict(feature_id=k, **(v if isinstance(v, dict) else {\"value\": v})) for k, v in data.items()]\n",
    "    assert isinstance(data, list), \"Expected list[dict] JSON structure\"\n",
    "    return data\n",
    "\n",
    "def extract_index_from_row(row: dict) -> int | None:\n",
    "    \"\"\"Extract feature index from various possible fields.\"\"\"\n",
    "    if 'index' in row and row['index'] is not None:\n",
    "        try:\n",
    "            return int(row['index'])\n",
    "        except Exception:\n",
    "            pass\n",
    "    fid = row.get('feature_id')\n",
    "    if isinstance(fid, str):\n",
    "        m = re.search(r'-(\\d+)$', fid)\n",
    "        if m:\n",
    "            try:\n",
    "                return int(m.group(1))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def compute_confusion_matrix(model_labels: list, human_labels: list) -> dict:\n",
    "    \"\"\"\n",
    "    Compute confusion matrix given model and human labels.\n",
    "    Returns dict with TP, TN, FP, FN.\n",
    "    Assumes 'related' = positive class, 'not-related' = negative class\n",
    "    \"\"\"\n",
    "    assert len(model_labels) == len(human_labels), \"Label lists must be same length\"\n",
    "    \n",
    "    tp = sum(1 for m, h in zip(model_labels, human_labels) if m == 'related' and h == 'related')\n",
    "    tn = sum(1 for m, h in zip(model_labels, human_labels) if m == 'not-related' and h == 'not-related')\n",
    "    fp = sum(1 for m, h in zip(model_labels, human_labels) if m == 'related' and h == 'not-related')\n",
    "    fn = sum(1 for m, h in zip(model_labels, human_labels) if m == 'not-related' and h == 'related')\n",
    "    \n",
    "    return {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn}\n",
    "\n",
    "print(\"Helper functions loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbcbec8",
   "metadata": {},
   "source": [
    "## Configuration: Model and Loss Data\n",
    "\n",
    "Configure which model to analyze and provide the experimental loss data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db51767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gemma-2-2b\n",
      "Baseline loss: 2.6\n",
      "Loss increase from ablating Alignment: 0.2800\n",
      "Loss increase from ablating Formatting (Style): 1.6100\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION: Select model and provide loss data\n",
    "# ============================================================\n",
    "\n",
    "# Choose model: 'gemma-2-2b' or 'gemma-2-9b'\n",
    "MODEL = 'gemma-2-2b'\n",
    "\n",
    "# Loss data (hardcoded from experimental results)\n",
    "# Format: {model: {'baseline_loss': X, 'alignment_loss': Y, 'formatting_loss': Z}}\n",
    "LOSS_DATA = {\n",
    "    'gemma-2-2b': {\n",
    "        'baseline_loss': 2.60,      # Baseline SimPO loss\n",
    "        'alignment_loss': 2.88,     # Loss after ablating alignment features\n",
    "        'formatting_loss': 4.21     # Loss after ablating formatting features\n",
    "    },\n",
    "    'gemma-2-9b': {\n",
    "        'baseline_loss': 2.50,      # Example values - UPDATE THESE\n",
    "        'alignment_loss': 2.70,     # Example values - UPDATE THESE\n",
    "        'formatting_loss': 3.80     # Example values - UPDATE THESE\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get loss data for selected model\n",
    "baseline_loss = LOSS_DATA[MODEL]['baseline_loss']\n",
    "alignment_loss = LOSS_DATA[MODEL]['alignment_loss']\n",
    "formatting_loss = LOSS_DATA[MODEL]['formatting_loss']\n",
    "\n",
    "# Calculate loss increases\n",
    "loss_increase_A = alignment_loss - baseline_loss\n",
    "loss_increase_S = formatting_loss - baseline_loss\n",
    "\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Baseline loss: {baseline_loss}\")\n",
    "print(f\"Loss increase from ablating Alignment: {loss_increase_A:.4f}\")\n",
    "print(f\"Loss increase from ablating Formatting (Style): {loss_increase_S:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905144e",
   "metadata": {},
   "source": [
    "## Step 1: Load Model Classifications and Count Features\n",
    "\n",
    "Load the DeepSeek classifications for alignment and formatting, and count the number of features predicted in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3826d0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model classifications for gemma-2-2b...\n",
      "Alignment file: 12-gemmascope-res-65k__l0-21_alignment_classified_deepseek-v3-0324.json\n",
      "Formatting file: 12-gemmascope-res-65k__l0-21_formatting_classified_deepseek-v3-0324.json\n",
      "\n",
      "Feature counts from model predictions:\n",
      "  Predicted as Alignment-related: 11143\n",
      "  Predicted as Formatting-related (Style): 15391\n",
      "  Total features: 65344\n",
      "\n",
      "Feature counts from model predictions:\n",
      "  Predicted as Alignment-related: 11143\n",
      "  Predicted as Formatting-related (Style): 15391\n",
      "  Total features: 65344\n"
     ]
    }
   ],
   "source": [
    "# Define file paths based on model selection\n",
    "base_path = Path('../../outputs/feature_classification')\n",
    "\n",
    "# Map model names to their file patterns\n",
    "MODEL_FILES = {\n",
    "    'gemma-2-2b': {\n",
    "        'alignment': base_path / 'gemma-2-2b' / '12-gemmascope-res-65k__l0-21_alignment_classified_deepseek-v3-0324.json',\n",
    "        'formatting': base_path / 'gemma-2-2b' / '12-gemmascope-res-65k__l0-21_formatting_classified_deepseek-v3-0324.json',\n",
    "        'human_alignment': base_path / 'human_labels' / '12-gemmascope-res-65k__l0-21_human_labels_alignment.json',\n",
    "        'human_formatting': base_path / 'human_labels' / '12-gemmascope-res-65k__l0-21_human_labels_formatting.json'\n",
    "    },\n",
    "    'gemma-2-9b': {\n",
    "        'alignment': base_path / 'gemma-2-9b' / '12-gemmascope-res-16k_canonical_alignment_classified_deepseek-deepseek-chat-v3-0324.json',\n",
    "        'formatting': base_path / 'gemma-2-9b' / '12-gemmascope-res-16k_canonical_formatting_classified_deepseek-deepseek-chat-v3-0324.json',\n",
    "        'human_alignment': base_path / 'human_labels' / '12-gemmascope-res-16k_canonical_human_labels_alignment.json',\n",
    "        'human_formatting': base_path / 'human_labels' / '12-gemmascope-res-16k_canonical_human_labels_formatting.json'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load model classifications\n",
    "alignment_file = MODEL_FILES[MODEL]['alignment']\n",
    "formatting_file = MODEL_FILES[MODEL]['formatting']\n",
    "\n",
    "print(f\"Loading model classifications for {MODEL}...\")\n",
    "print(f\"Alignment file: {alignment_file.name}\")\n",
    "print(f\"Formatting file: {formatting_file.name}\")\n",
    "\n",
    "# Load and process alignment classifications\n",
    "alignment_records = load_json_records(alignment_file)\n",
    "alignment_df = pd.DataFrame(alignment_records)\n",
    "alignment_df['feature_index'] = alignment_df.apply(lambda r: extract_index_from_row(r), axis=1)\n",
    "\n",
    "# Load and process formatting classifications\n",
    "formatting_records = load_json_records(formatting_file)\n",
    "formatting_df = pd.DataFrame(formatting_records)\n",
    "formatting_df['feature_index'] = formatting_df.apply(lambda r: extract_index_from_row(r), axis=1)\n",
    "\n",
    "# Count features predicted as positive in each category\n",
    "N_predicted_A = (alignment_df['label'] == 'related').sum()\n",
    "N_predicted_S = (formatting_df['label'] == 'related').sum()\n",
    "\n",
    "print(f\"\\nFeature counts from model predictions:\")\n",
    "print(f\"  Predicted as Alignment-related: {N_predicted_A}\")\n",
    "print(f\"  Predicted as Formatting-related (Style): {N_predicted_S}\")\n",
    "print(f\"  Total features: {len(alignment_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562b3be",
   "metadata": {},
   "source": [
    "## Step 2: Load Human Labels and Compute Confusion Matrices\n",
    "\n",
    "Load human validation labels and compute confusion matrices to estimate classification accuracy and contamination rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f017021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading human validation labels...\n",
      "Human alignment file: 12-gemmascope-res-65k__l0-21_human_labels_alignment.json\n",
      "Human formatting file: 12-gemmascope-res-65k__l0-21_human_labels_formatting.json\n",
      "\n",
      "Human labeled samples:\n",
      "  Alignment: 300 samples\n",
      "  Formatting: 300 samples\n",
      "  Sample alignment indices: [5475, 6903, 14974, 47733, 59708]\n",
      "  Sample formatting indices: [5475, 6903, 14974, 47733, 59708]\n",
      "\n",
      "Model data sample indices:\n",
      "  Alignment: [53844, 53847, 53890, 53924, 53896, 53848, 53925, 53859, 53886, 53885]\n",
      "  Formatting: [53844, 53847, 53890, 53924, 53896, 53848, 53925, 53859, 53886, 53885]\n",
      "\n",
      "Model predictions for human-labeled features:\n",
      "  Alignment: 300 samples\n",
      "  Formatting: 300 samples\n",
      "\n",
      "Validation sample sizes (after merge):\n",
      "  Alignment: 300 samples\n",
      "  Formatting: 300 samples\n",
      "\n",
      "Confusion Matrix for Alignment Classifier:\n",
      "  TP (Correct Positive): 27\n",
      "  TN (Correct Negative): 227\n",
      "  FP (False Positive): 22\n",
      "  FN (False Negative): 24\n",
      "  Accuracy: 84.67%\n",
      "\n",
      "Confusion Matrix for Formatting Classifier:\n",
      "  TP (Correct Positive): 69\n",
      "  TN (Correct Negative): 201\n",
      "  FP (False Positive): 5\n",
      "  FN (False Negative): 25\n",
      "  Accuracy: 90.00%\n",
      "\n",
      "Model data sample indices:\n",
      "  Alignment: [53844, 53847, 53890, 53924, 53896, 53848, 53925, 53859, 53886, 53885]\n",
      "  Formatting: [53844, 53847, 53890, 53924, 53896, 53848, 53925, 53859, 53886, 53885]\n",
      "\n",
      "Model predictions for human-labeled features:\n",
      "  Alignment: 300 samples\n",
      "  Formatting: 300 samples\n",
      "\n",
      "Validation sample sizes (after merge):\n",
      "  Alignment: 300 samples\n",
      "  Formatting: 300 samples\n",
      "\n",
      "Confusion Matrix for Alignment Classifier:\n",
      "  TP (Correct Positive): 27\n",
      "  TN (Correct Negative): 227\n",
      "  FP (False Positive): 22\n",
      "  FN (False Negative): 24\n",
      "  Accuracy: 84.67%\n",
      "\n",
      "Confusion Matrix for Formatting Classifier:\n",
      "  TP (Correct Positive): 69\n",
      "  TN (Correct Negative): 201\n",
      "  FP (False Positive): 5\n",
      "  FN (False Negative): 25\n",
      "  Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "# Load human labels\n",
    "human_alignment_file = MODEL_FILES[MODEL]['human_alignment']\n",
    "human_formatting_file = MODEL_FILES[MODEL]['human_formatting']\n",
    "\n",
    "print(f\"Loading human validation labels...\")\n",
    "print(f\"Human alignment file: {human_alignment_file.name}\")\n",
    "print(f\"Human formatting file: {human_formatting_file.name}\")\n",
    "\n",
    "# Load human alignment labels\n",
    "human_alignment_records = load_json_records(human_alignment_file)\n",
    "human_alignment_df = pd.DataFrame(human_alignment_records)\n",
    "\n",
    "# Load human formatting labels\n",
    "human_formatting_records = load_json_records(human_formatting_file)\n",
    "human_formatting_df = pd.DataFrame(human_formatting_records)\n",
    "\n",
    "# Clean human data - use the 'feature_index' field directly if it exists\n",
    "if 'feature_index' in human_alignment_df.columns:\n",
    "    human_alignment_df_clean = human_alignment_df[['feature_index', 'label']].copy()\n",
    "    human_alignment_df_clean = human_alignment_df_clean.dropna(subset=['feature_index'])\n",
    "    human_alignment_df_clean['feature_index'] = human_alignment_df_clean['feature_index'].astype(int)\n",
    "else:\n",
    "    raise ValueError(\"Human alignment data missing 'feature_index' column\")\n",
    "\n",
    "if 'feature_index' in human_formatting_df.columns:\n",
    "    human_formatting_df_clean = human_formatting_df[['feature_index', 'label']].copy()\n",
    "    human_formatting_df_clean = human_formatting_df_clean.dropna(subset=['feature_index'])\n",
    "    human_formatting_df_clean['feature_index'] = human_formatting_df_clean['feature_index'].astype(int)\n",
    "else:\n",
    "    raise ValueError(\"Human formatting data missing 'feature_index' column\")\n",
    "\n",
    "print(f\"\\nHuman labeled samples:\")\n",
    "print(f\"  Alignment: {len(human_alignment_df_clean)} samples\")\n",
    "print(f\"  Formatting: {len(human_formatting_df_clean)} samples\")\n",
    "\n",
    "# Debug: Show sample of human indices\n",
    "if len(human_alignment_df_clean) > 0:\n",
    "    print(f\"  Sample alignment indices: {sorted(human_alignment_df_clean['feature_index'].head(5).tolist())}\")\n",
    "if len(human_formatting_df_clean) > 0:\n",
    "    print(f\"  Sample formatting indices: {sorted(human_formatting_df_clean['feature_index'].head(5).tolist())}\")\n",
    "\n",
    "# Get the feature indices that were human-labeled\n",
    "human_labeled_indices_A = set(human_alignment_df_clean['feature_index'].tolist())\n",
    "human_labeled_indices_S = set(human_formatting_df_clean['feature_index'].tolist())\n",
    "\n",
    "# Extract feature indices from model data and ensure they're integers\n",
    "alignment_df['feature_index'] = alignment_df.apply(lambda r: extract_index_from_row(r), axis=1)\n",
    "formatting_df['feature_index'] = formatting_df.apply(lambda r: extract_index_from_row(r), axis=1)\n",
    "\n",
    "# Debug: Show sample of model indices\n",
    "alignment_sample_indices = alignment_df['feature_index'].dropna().head(10).astype(int).tolist()\n",
    "formatting_sample_indices = formatting_df['feature_index'].dropna().head(10).astype(int).tolist()\n",
    "print(f\"\\nModel data sample indices:\")\n",
    "print(f\"  Alignment: {alignment_sample_indices}\")\n",
    "print(f\"  Formatting: {formatting_sample_indices}\")\n",
    "\n",
    "# Filter model predictions to only include human-labeled features\n",
    "# For alignment\n",
    "alignment_df_filtered = alignment_df[alignment_df['feature_index'].notna()].copy()\n",
    "alignment_df_filtered['feature_index'] = alignment_df_filtered['feature_index'].astype(int)\n",
    "alignment_df_filtered = alignment_df_filtered[alignment_df_filtered['feature_index'].isin(human_labeled_indices_A)]\n",
    "alignment_df_filtered = alignment_df_filtered[['feature_index', 'label']]\n",
    "\n",
    "# For formatting\n",
    "formatting_df_filtered = formatting_df[formatting_df['feature_index'].notna()].copy()\n",
    "formatting_df_filtered['feature_index'] = formatting_df_filtered['feature_index'].astype(int)\n",
    "formatting_df_filtered = formatting_df_filtered[formatting_df_filtered['feature_index'].isin(human_labeled_indices_S)]\n",
    "formatting_df_filtered = formatting_df_filtered[['feature_index', 'label']]\n",
    "\n",
    "print(f\"\\nModel predictions for human-labeled features:\")\n",
    "print(f\"  Alignment: {len(alignment_df_filtered)} samples\")\n",
    "print(f\"  Formatting: {len(formatting_df_filtered)} samples\")\n",
    "\n",
    "# Join model predictions with human labels for alignment\n",
    "merged_alignment = alignment_df_filtered.merge(\n",
    "    human_alignment_df_clean,\n",
    "    on='feature_index',\n",
    "    how='inner',\n",
    "    suffixes=('_model', '_human')\n",
    ")\n",
    "\n",
    "# Join model predictions with human labels for formatting\n",
    "merged_formatting = formatting_df_filtered.merge(\n",
    "    human_formatting_df_clean,\n",
    "    on='feature_index',\n",
    "    how='inner',\n",
    "    suffixes=('_model', '_human')\n",
    ")\n",
    "\n",
    "print(f\"\\nValidation sample sizes (after merge):\")\n",
    "print(f\"  Alignment: {len(merged_alignment)} samples\")\n",
    "print(f\"  Formatting: {len(merged_formatting)} samples\")\n",
    "\n",
    "# Compute confusion matrices\n",
    "cm_A = compute_confusion_matrix(\n",
    "    merged_alignment['label_model'].tolist(),\n",
    "    merged_alignment['label_human'].tolist()\n",
    ")\n",
    "\n",
    "cm_S = compute_confusion_matrix(\n",
    "    merged_formatting['label_model'].tolist(),\n",
    "    merged_formatting['label_human'].tolist()\n",
    ")\n",
    "\n",
    "# Extract values\n",
    "TP_A, TN_A, FP_A, FN_A = cm_A['TP'], cm_A['TN'], cm_A['FP'], cm_A['FN']\n",
    "TP_S, TN_S, FP_S, FN_S = cm_S['TP'], cm_S['TN'], cm_S['FP'], cm_S['FN']\n",
    "\n",
    "print(f\"\\nConfusion Matrix for Alignment Classifier:\")\n",
    "print(f\"  TP (Correct Positive): {TP_A}\")\n",
    "print(f\"  TN (Correct Negative): {TN_A}\")\n",
    "print(f\"  FP (False Positive): {FP_A}\")\n",
    "print(f\"  FN (False Negative): {FN_A}\")\n",
    "total_A = TP_A + TN_A + FP_A + FN_A\n",
    "if total_A > 0:\n",
    "    print(f\"  Accuracy: {(TP_A + TN_A) / total_A:.2%}\")\n",
    "else:\n",
    "    print(f\"  Accuracy: N/A (no samples)\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix for Formatting Classifier:\")\n",
    "print(f\"  TP (Correct Positive): {TP_S}\")\n",
    "print(f\"  TN (Correct Negative): {TN_S}\")\n",
    "print(f\"  FP (False Positive): {FP_S}\")\n",
    "print(f\"  FN (False Negative): {FN_S}\")\n",
    "total_S = TP_S + TN_S + FP_S + FN_S\n",
    "if total_S > 0:\n",
    "    print(f\"  Accuracy: {(TP_S + TN_S) / total_S:.2%}\")\n",
    "else:\n",
    "    print(f\"  Accuracy: N/A (no samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff400a5",
   "metadata": {},
   "source": [
    "## Step 3: Calculate Naive Loss Per Feature (LPF)\n",
    "\n",
    "Calculate the naive (uncorrected) loss per feature by simply dividing the total loss increase by the number of predicted features in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd552be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive LPF for Alignment: 2.51e-05\n",
      "Naive LPF for Formatting: 1.05e-04\n",
      "Naive Ratio: 4.16x\n"
     ]
    }
   ],
   "source": [
    "# Calculate naive LPF (no correction for contamination)\n",
    "lpf_A_naive = loss_increase_A / N_predicted_A\n",
    "lpf_S_naive = loss_increase_S / N_predicted_S\n",
    "\n",
    "print(\"Naive LPF for Alignment: {:.2e}\".format(lpf_A_naive))\n",
    "print(\"Naive LPF for Formatting: {:.2e}\".format(lpf_S_naive))\n",
    "print(\"Naive Ratio: {:.2f}x\".format(lpf_S_naive / lpf_A_naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1066bb",
   "metadata": {},
   "source": [
    "## Step 4: Estimate Contamination in Each Bucket\n",
    "\n",
    "Use the False Positive Rate from the confusion matrix to estimate how many features in each predicted category are actually contaminants from the other category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5933a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alignment FPR: 44.90% | Pure: 6140, Contaminants: 5003\n",
      "Formatting FPR: 6.76% | Pure: 14351, Contaminants: 1040\n"
     ]
    }
   ],
   "source": [
    "# Calculate contamination rates using False Positive Rate\n",
    "# FPR = FP / (TP + FP) - the fraction of positive predictions that are wrong\n",
    "\n",
    "# For Alignment bucket\n",
    "fp_rate_A = FP_A / (TP_A + FP_A) if (TP_A + FP_A) > 0 else 0\n",
    "num_contaminants_in_A = N_predicted_A * fp_rate_A\n",
    "num_pure_A = N_predicted_A - num_contaminants_in_A\n",
    "\n",
    "# For Formatting bucket\n",
    "fp_rate_S = FP_S / (TP_S + FP_S) if (TP_S + FP_S) > 0 else 0\n",
    "num_contaminants_in_S = N_predicted_S * fp_rate_S\n",
    "num_pure_S = N_predicted_S - num_contaminants_in_S\n",
    "\n",
    "print(f\"\\nAlignment FPR: {fp_rate_A:.2%} | Pure: {num_pure_A:.0f}, Contaminants: {num_contaminants_in_A:.0f}\")\n",
    "print(f\"Formatting FPR: {fp_rate_S:.2%} | Pure: {num_pure_S:.0f}, Contaminants: {num_contaminants_in_S:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ea6ce",
   "metadata": {},
   "source": [
    "## Step 5: Solve System of Equations for Corrected LPF\n",
    "\n",
    "Set up and solve a system of two linear equations to estimate the \"pure\" LPF for each category:\n",
    "\n",
    "**Equation 1** (Alignment bucket):  \n",
    "`loss_increase_A = (num_pure_A × lpf_pure_A) + (num_contaminants_in_A × lpf_pure_S)`\n",
    "\n",
    "**Equation 2** (Formatting bucket):  \n",
    "`loss_increase_S = (num_pure_S × lpf_pure_S) + (num_contaminants_in_S × lpf_pure_A)`\n",
    "\n",
    "Solve for `lpf_pure_A` and `lpf_pure_S`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19aca3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corrected LPF for Alignment: -4.87e-05\n",
      "Corrected LPF for Formatting: 1.16e-04\n",
      "Corrected Ratio: N/A (alignment LPF is negative)\n"
     ]
    }
   ],
   "source": [
    "# Set up the system of equations: Ax = b\n",
    "# where x = [lpf_pure_A, lpf_pure_S]\n",
    "\n",
    "# Coefficient matrix A\n",
    "A = np.array([\n",
    "    [num_pure_A, num_contaminants_in_A],      # Equation 1\n",
    "    [num_contaminants_in_S, num_pure_S]        # Equation 2\n",
    "])\n",
    "\n",
    "# Right-hand side vector b\n",
    "b = np.array([loss_increase_A, loss_increase_S])\n",
    "\n",
    "# Solve the system\n",
    "try:\n",
    "    lpf_corrected = np.linalg.solve(A, b)\n",
    "    lpf_pure_A = lpf_corrected[0]\n",
    "    lpf_pure_S = lpf_corrected[1]\n",
    "    \n",
    "    print(f\"\\nCorrected LPF for Alignment: {lpf_pure_A:.2e}\")\n",
    "    print(f\"Corrected LPF for Formatting: {lpf_pure_S:.2e}\")\n",
    "    if lpf_pure_A > 0:\n",
    "        print(f\"Corrected Ratio: {lpf_pure_S / lpf_pure_A:.2f}x\")\n",
    "    else:\n",
    "        print(f\"Corrected Ratio: N/A (alignment LPF is negative)\")\n",
    "    \n",
    "except np.linalg.LinAlgError as e:\n",
    "    print(f\"\\nERROR: Could not solve the system. {e}\")\n",
    "    lpf_pure_A = None\n",
    "    lpf_pure_S = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c2b184",
   "metadata": {},
   "source": [
    "## Step 6: Summary Comparison\n",
    "\n",
    "Compare the naive (uncorrected) results with the corrected results side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f16a1470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "                 Metric Naive (Uncorrected)     Corrected (Pure)\n",
      "          LPF Alignment            2.51e-05            -4.87e-05\n",
      " LPF Formatting (Style)            1.05e-04             1.16e-04\n",
      "Ratio (Style/Alignment)               4.16x N/A (negative denom)\n",
      "\n",
      "============================================================\n",
      "INTERPRETATION\n",
      "============================================================\n",
      "After correcting for classification noise:\n",
      "• Formatting features: 1.16e-04 LPF (positive impact on loss)\n",
      "• Alignment features: -4.87e-05 LPF (negative - suggests little/no impact)\n",
      "\n",
      "The correction reveals that contamination by high-impact formatting features\n",
      "was inflating the apparent impact of alignment features. Pure alignment features\n",
      "have minimal or negative impact, while formatting features remain highly impactful.\n"
     ]
    }
   ],
   "source": [
    "if lpf_pure_A is not None and lpf_pure_S is not None:\n",
    "    # Create comparison table\n",
    "    comparison = pd.DataFrame({\n",
    "        'Metric': [\n",
    "            'LPF Alignment',\n",
    "            'LPF Formatting (Style)',\n",
    "            'Ratio (Style/Alignment)'\n",
    "        ],\n",
    "        'Naive (Uncorrected)': [\n",
    "            f'{lpf_A_naive:.2e}',\n",
    "            f'{lpf_S_naive:.2e}',\n",
    "            f'{lpf_S_naive / lpf_A_naive:.2f}x'\n",
    "        ],\n",
    "        'Corrected (Pure)': [\n",
    "            f'{lpf_pure_A:.2e}',\n",
    "            f'{lpf_pure_S:.2e}',\n",
    "            f'{lpf_pure_S / lpf_pure_A:.2f}x' if lpf_pure_A > 0 else 'N/A (negative denom)'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(comparison.to_string(index=False))\n",
    "    \n",
    "    # Interpretation based on actual results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INTERPRETATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if lpf_pure_A < 0 and lpf_pure_S > 0:\n",
    "        print(\"After correcting for classification noise:\")\n",
    "        print(f\"• Formatting features: {lpf_pure_S:.2e} LPF (positive impact on loss)\")\n",
    "        print(f\"• Alignment features: {lpf_pure_A:.2e} LPF (negative - suggests little/no impact)\")\n",
    "        print(\"\\nThe correction reveals that contamination by high-impact formatting features\")\n",
    "        print(\"was inflating the apparent impact of alignment features. Pure alignment features\")\n",
    "        print(\"have minimal or negative impact, while formatting features remain highly impactful.\")\n",
    "    elif lpf_pure_A > 0 and lpf_pure_S > 0:\n",
    "        ratio_change = (lpf_pure_S / lpf_pure_A) / (lpf_S_naive / lpf_A_naive)\n",
    "        if ratio_change > 1.1:\n",
    "            print(\"After correction, the formatting/alignment ratio increased,\")\n",
    "            print(\"suggesting the gap is larger than initially estimated.\")\n",
    "        elif ratio_change < 0.9:\n",
    "            print(\"After correction, the formatting/alignment ratio decreased,\")\n",
    "            print(\"suggesting the gap was overestimated due to classification noise.\")\n",
    "        else:\n",
    "            print(\"After correction, the formatting/alignment ratio remains similar.\")\n",
    "    else:\n",
    "        print(\"Unexpected result pattern - manual interpretation required.\")\n",
    "else:\n",
    "    print(\"Could not compute comparison due to solver error.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
