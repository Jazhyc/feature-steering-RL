{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d731c725",
   "metadata": {},
   "source": [
    "# Contrastive Activation Addition (CAA) Baseline\n",
    "\n",
    "This notebook implements a baseline for comparison using Contrastive Activation Addition (CAA).\n",
    "We use contrast pairs from the UltraFeedback dataset to generate steering vectors and observe the impact on SimPO loss.\n",
    "\n",
    "We compare two steering methods:\n",
    "1.  **Residual Stream Steering**: Computing the difference vector in the residual stream directly.\n",
    "2.  **SAE Feature Steering**: Computing the difference vector in the SAE latent space and decoding it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fedb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath(\"../../src\"))\n",
    "\n",
    "from fsrl.simPO import apply_chat_template\n",
    "from fsrl.simPO.simpo_config import SimPOConfig\n",
    "\n",
    "# Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"gemma-2-2b-it\"\n",
    "SAE_RELEASE = \"gemma-scope-2b-pt-res\"\n",
    "SAE_ID = \"layer_12/width_65k/average_l0_72\"\n",
    "LAYER = 12\n",
    "HOOK_NAME = f\"blocks.{LAYER}.hook_resid_post\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c62325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66138ebc87474f0190d9801894c7ba86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device=DEVICE,\n",
    "    dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "# Load SAE\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=SAE_RELEASE,\n",
    "    sae_id=SAE_ID,\n",
    "    device=DEVICE\n",
    ")\n",
    "sae = sae.to(dtype=torch.bfloat16) # Ensure dtype matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d7f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subset size: 150\n",
      "Eval dataset size: 1961\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "dataset_name = \"princeton-nlp/llama3-ultrafeedback-armorm\"\n",
    "train_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "eval_dataset = load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "# Filter/Sample\n",
    "# We'll use a subset of train for generating the steering vector\n",
    "num_samples = 150\n",
    "train_subset = train_dataset.shuffle(seed=42).select(range(num_samples))\n",
    "\n",
    "print(f\"Train subset size: {len(train_subset)}\")\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Chat Template\n",
    "# Using the one from the config file provided in context\n",
    "chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] | trim + '\\n\\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{% for message in messages %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + content | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "\n",
    "model.tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing steering vectors...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7593275d7c64f90847cd30256838ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_response_activations(model, prompt, response, layer_hook):\n",
    "    \"\"\"\n",
    "    Get the mean activation of the response tokens.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def process_example(example, tokenizer):\n",
    "    # Extract prompt and responses\n",
    "    chosen_msgs = example['chosen'] # List of dicts\n",
    "    rejected_msgs = example['rejected']\n",
    "    \n",
    "    # The prompt is everything except the last message\n",
    "    prompt_msgs = chosen_msgs[:-1]\n",
    "    chosen_response = chosen_msgs[-1]['content']\n",
    "    rejected_response = rejected_msgs[-1]['content']\n",
    "    \n",
    "    prompt_text = tokenizer.apply_chat_template(prompt_msgs, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Full text\n",
    "    chosen_text = prompt_text + chosen_response + \"<end_of_turn>\"\n",
    "    rejected_text = prompt_text + rejected_response + \"<end_of_turn>\"\n",
    "    \n",
    "    return prompt_text, chosen_text, rejected_text\n",
    "\n",
    "def get_steering_vectors(model, sae, dataset, hook_name, batch_size=1):\n",
    "    # Initialize accumulators\n",
    "    resid_diff_sum = None\n",
    "    sae_diff_sum = None\n",
    "    count = 0\n",
    "    \n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        \n",
    "        prompts = []\n",
    "        chosens = []\n",
    "        rejecteds = []\n",
    "        \n",
    "        for j in range(len(batch['chosen'])):\n",
    "            p, c, r = process_example({\n",
    "                'chosen': batch['chosen'][j], \n",
    "                'rejected': batch['rejected'][j]\n",
    "            }, model.tokenizer)\n",
    "            prompts.append(p)\n",
    "            chosens.append(c)\n",
    "            rejecteds.append(r)\n",
    "            \n",
    "        for p, c, r in zip(prompts, chosens, rejecteds):\n",
    "            # Tokenize\n",
    "            p_tokens = model.to_tokens(p)\n",
    "            c_tokens = model.to_tokens(c)\n",
    "            r_tokens = model.to_tokens(r)\n",
    "            \n",
    "            # Run model for chosen\n",
    "            with torch.no_grad():\n",
    "                _, cache_c = model.run_with_cache(c_tokens, names_filter=hook_name)\n",
    "                # Use only the last token activation\n",
    "                act_c = cache_c[hook_name][0, -1, :] \n",
    "                \n",
    "                # Run model for rejected\n",
    "                _, cache_r = model.run_with_cache(r_tokens, names_filter=hook_name)\n",
    "                # Use only the last token activation\n",
    "                act_r = cache_r[hook_name][0, -1, :]\n",
    "                \n",
    "                # Residual Diff\n",
    "                diff = act_c - act_r\n",
    "                \n",
    "                # SAE Diff\n",
    "                # Encode the last token activation\n",
    "                sae_act_c = sae.encode(act_c) \n",
    "                sae_act_r = sae.encode(act_r)\n",
    "                \n",
    "                sae_diff = sae_act_c - sae_act_r\n",
    "\n",
    "                # Accumulate\n",
    "                if resid_diff_sum is None:\n",
    "                    resid_diff_sum = torch.zeros_like(diff)\n",
    "                    sae_diff_sum = torch.zeros_like(sae_diff)\n",
    "                \n",
    "                resid_diff_sum += diff\n",
    "                sae_diff_sum += sae_diff\n",
    "                count += 1\n",
    "                \n",
    "        # Clear cache to free memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Average over dataset\n",
    "    avg_resid_diff = resid_diff_sum / count\n",
    "    avg_sae_diff = sae_diff_sum / count\n",
    "    \n",
    "    return avg_resid_diff, avg_sae_diff\n",
    "\n",
    "print(\"Computing steering vectors...\")\n",
    "# Use train_subset for steering vectors\n",
    "vec_resid, vec_sae_latent = get_steering_vectors(model, sae, train_subset, HOOK_NAME)\n",
    "\n",
    "# Decode SAE vector\n",
    "vec_sae_recon = sae.decode(vec_sae_latent)\n",
    "\n",
    "print(\"Vectors computed.\")\n",
    "print(f\"Resid vector norm: {vec_resid.norm().item()}\")\n",
    "print(f\"SAE recon vector norm: {vec_sae_recon.norm().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903ac158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimPO Loss Function\n",
    "def compute_simpo_loss(model, batch, beta=10.0, gamma_beta_ratio=0.5):\n",
    "    # Prepare batch\n",
    "    # We need to compute logps for chosen and rejected\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    # Iterate over the dataset\n",
    "    for i in range(len(batch['chosen'])):\n",
    "        p, c, r = process_example({\n",
    "            'chosen': batch['chosen'][i], \n",
    "            'rejected': batch['rejected'][i]\n",
    "        }, model.tokenizer)\n",
    "        \n",
    "        # Tokenize\n",
    "        c_tokens = model.to_tokens(c)\n",
    "        r_tokens = model.to_tokens(r)\n",
    "        p_tokens = model.to_tokens(p)\n",
    "        p_len = p_tokens.shape[1]\n",
    "        \n",
    "        # Labels: mask prompt with -100\n",
    "        c_labels = c_tokens.clone()\n",
    "        c_labels[0, :p_len] = -100\n",
    "        \n",
    "        r_labels = r_tokens.clone()\n",
    "        r_labels[0, :p_len] = -100\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Chosen\n",
    "            logits_c = model(c_tokens)\n",
    "            # Shift logits and labels\n",
    "            shift_logits_c = logits_c[..., :-1, :].contiguous()\n",
    "            shift_labels_c = c_labels[..., 1:].contiguous()\n",
    "            \n",
    "            # Compute log probs\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "            # We want log probs of the correct tokens\n",
    "            # CrossEntropyLoss returns -log_prob\n",
    "            token_losses_c = loss_fct(shift_logits_c.view(-1, shift_logits_c.size(-1)), shift_labels_c.view(-1))\n",
    "            token_losses_c = token_losses_c.view(c_labels.size(0), -1)\n",
    "            \n",
    "            # Sum log probs over non-masked tokens\n",
    "            mask_c = (shift_labels_c != -100)\n",
    "            log_prob_c = -token_losses_c * mask_c.float()\n",
    "            sum_log_prob_c = log_prob_c.sum(dim=1)\n",
    "            avg_log_prob_c = sum_log_prob_c / mask_c.sum(dim=1)\n",
    "            \n",
    "            # Rejected\n",
    "            logits_r = model(r_tokens)\n",
    "            shift_logits_r = logits_r[..., :-1, :].contiguous()\n",
    "            shift_labels_r = r_labels[..., 1:].contiguous()\n",
    "            \n",
    "            token_losses_r = loss_fct(shift_logits_r.view(-1, shift_logits_r.size(-1)), shift_labels_r.view(-1))\n",
    "            token_losses_r = token_losses_r.view(r_labels.size(0), -1)\n",
    "            \n",
    "            mask_r = (shift_labels_r != -100)\n",
    "            log_prob_r = -token_losses_r * mask_r.float()\n",
    "            sum_log_prob_r = log_prob_r.sum(dim=1)\n",
    "            avg_log_prob_r = sum_log_prob_r / mask_r.sum(dim=1)\n",
    "            \n",
    "            # SimPO Loss\n",
    "            pi_logratios = avg_log_prob_c - avg_log_prob_r\n",
    "            \n",
    "            logits = pi_logratios - gamma_beta_ratio\n",
    "            loss = -F.logsigmoid(beta * logits)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    return np.mean(losses)\n",
    "\n",
    "# Evaluation Loop\n",
    "def evaluate_steering(model, eval_dataset, steering_vec, hook_name, coeffs):\n",
    "    results = []\n",
    "    \n",
    "    # Evaluate on the whole validation set\n",
    "    \n",
    "    for coeff in tqdm(coeffs, desc=\"Evaluating coefficients\"):\n",
    "        \n",
    "        # Define hook\n",
    "        def steering_hook(resid, hook):\n",
    "            return resid + coeff * steering_vec\n",
    "            \n",
    "        # Add hook\n",
    "        model.add_hook(hook_name, steering_hook)\n",
    "        \n",
    "        try:\n",
    "            loss = compute_simpo_loss(model, eval_dataset)\n",
    "            results.append(loss)\n",
    "        finally:\n",
    "            model.reset_hooks()\n",
    "            \n",
    "    return results\n",
    "\n",
    "coeffs = [-10, -5, -2, -1, 0, 1, 2, 5, 10] \n",
    "\n",
    "print(\"Evaluating Residual Steering...\")\n",
    "resid_results = evaluate_steering(model, eval_dataset, vec_resid, HOOK_NAME, coeffs)\n",
    "\n",
    "print(\"Evaluating SAE Steering...\")\n",
    "sae_results = evaluate_steering(model, eval_dataset, vec_sae_recon, HOOK_NAME, coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d2798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(coeffs, resid_results, label='Residual Steering', marker='o')\n",
    "plt.plot(coeffs, sae_results, label='SAE Feature Steering', marker='x')\n",
    "plt.xlabel('Steering Coefficient')\n",
    "plt.ylabel('SimPO Loss')\n",
    "plt.title('Impact of Steering on SimPO Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
