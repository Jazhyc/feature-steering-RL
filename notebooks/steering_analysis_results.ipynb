{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a63472",
   "metadata": {},
   "source": [
    "# Steering Analysis Results: Statistical Significance Testing\n",
    "\n",
    "This notebook analyzes all human-model comparison metrics from the comprehensive steering experiments, calculating p-values and statistical significance for model performance differences.\n",
    "\n",
    "## Overview:\n",
    "- **Data Source**: JSON metrics files from `outputs/feature_classification/metrics/`\n",
    "- **Statistical Tests**: Binomial tests for accuracy, confidence intervals for correlations\n",
    "- **Output**: Comprehensive table with p-values and significance indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af1854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from scipy.stats import binom, norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1340a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Metrics Files\n",
    "metrics_dir = Path('../outputs/feature_classification/metrics')\n",
    "print(f\"Looking for metrics files in: {metrics_dir.absolute()}\")\n",
    "\n",
    "# Find all JSON metrics files\n",
    "metrics_files = list(metrics_dir.glob('*_metrics.json'))\n",
    "print(f\"Found {len(metrics_files)} metrics files:\")\n",
    "for f in sorted(metrics_files):\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "if not metrics_files:\n",
    "    print(\"\\n⚠️  No metrics files found! Make sure to run the steering analysis experiments first.\")\n",
    "    print(\"   Run: python src/fsrl/scripts/alignment_steering_analysis.py --run_all_experiments\")\n",
    "else:\n",
    "    print(f\"\\n✅ Ready to analyze {len(metrics_files)} experiment results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and Structure Data\n",
    "all_metrics = []\n",
    "\n",
    "for metrics_file in metrics_files:\n",
    "    try:\n",
    "        with metrics_file.open('r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract key information from filename\n",
    "        filename = metrics_file.stem\n",
    "        # Expected format: \"human_labels_MODE_vs_MODEL_metrics\"\n",
    "        parts = filename.replace('_metrics', '').split('_vs_')\n",
    "        if len(parts) == 2:\n",
    "            human_part = parts[0]  # e.g., \"12-gemmascope-res-65k__l0-21_human_labels_alignment\"\n",
    "            model_part = parts[1]  # e.g., \"12-gemmascope-res-65k__l0-21_alignment_classified_deepseek-v3-0324\"\n",
    "            \n",
    "            # Extract mode from human_part\n",
    "            if '_alignment' in human_part:\n",
    "                mode = 'alignment'\n",
    "            elif '_formatting' in human_part:\n",
    "                mode = 'formatting'\n",
    "            else:\n",
    "                mode = 'unknown'\n",
    "            \n",
    "            # Extract model name from model_part\n",
    "            if '_classified_' in model_part:\n",
    "                model_name = model_part.split('_classified_')[-1]\n",
    "            else:\n",
    "                model_name = model_part\n",
    "        else:\n",
    "            mode = 'unknown'\n",
    "            model_name = filename\n",
    "        \n",
    "        # Structure the data\n",
    "        metrics_record = {\n",
    "            'filename': filename,\n",
    "            'mode': mode,\n",
    "            'model_name': model_name,\n",
    "            'samples': data.get('samples', 0),\n",
    "            'accuracy': data.get('accuracy', None),\n",
    "            'phi_correlation': data.get('phi', None),\n",
    "            'tp': data.get('tp', 0),\n",
    "            'tn': data.get('tn', 0),\n",
    "            'fp': data.get('fp', 0),\n",
    "            'fn': data.get('fn', 0),\n",
    "            'timestamp': data.get('timestamp', ''),\n",
    "            'human_labels_path': data.get('human_labels', ''),\n",
    "            'model_classifications_path': data.get('model_classifications', '')\n",
    "        }\n",
    "        all_metrics.append(metrics_record)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {metrics_file.name}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_metrics)\n",
    "print(f\"\\nLoaded {len(df)} experiment results:\")\n",
    "if len(df) > 0:\n",
    "    print(f\"Modes: {df['mode'].value_counts().to_dict()}\")\n",
    "    print(f\"Models: {df['model_name'].nunique()} unique models\")\n",
    "    print(f\"Sample sizes: {df['samples'].min()} - {df['samples'].max()}\")\n",
    "    \n",
    "# Display first few rows\n",
    "print(\"\\nFirst few records:\")\n",
    "display(df[['mode', 'model_name', 'samples', 'accuracy', 'phi_correlation']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18011dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Statistical Tests\n",
    "def calculate_accuracy_pvalue(tp, tn, fp, fn, null_accuracy=0.5):\n",
    "    \"\"\"Calculate p-value for accuracy being significantly different from null hypothesis.\"\"\"\n",
    "    total = tp + tn + fp + fn\n",
    "    correct = tp + tn\n",
    "    if total == 0:\n",
    "        return None\n",
    "    # Two-tailed binomial test\n",
    "    p_value = binom.cdf(correct, total, null_accuracy)\n",
    "    p_value = 2 * min(p_value, 1 - p_value)  # Two-tailed\n",
    "    return p_value\n",
    "\n",
    "def calculate_phi_confidence_interval(phi, n, confidence=0.95):\n",
    "    \"\"\"Calculate confidence interval for phi coefficient.\"\"\"\n",
    "    if phi is None or n <= 0:\n",
    "        return None, None\n",
    "    \n",
    "    # Fisher z-transformation\n",
    "    z = 0.5 * np.log((1 + phi) / (1 - phi)) if abs(phi) < 0.999 else None\n",
    "    if z is None:\n",
    "        return None, None\n",
    "    \n",
    "    se = 1 / np.sqrt(n - 3)\n",
    "    alpha = 1 - confidence\n",
    "    z_crit = norm.ppf(1 - alpha/2)\n",
    "    \n",
    "    ci_z_lower = z - z_crit * se\n",
    "    ci_z_upper = z + z_crit * se\n",
    "    \n",
    "    # Transform back\n",
    "    ci_lower = (np.exp(2 * ci_z_lower) - 1) / (np.exp(2 * ci_z_lower) + 1)\n",
    "    ci_upper = (np.exp(2 * ci_z_upper) - 1) / (np.exp(2 * ci_z_upper) + 1)\n",
    "    \n",
    "    return ci_lower, ci_upper\n",
    "\n",
    "def significance_level(p_value):\n",
    "    \"\"\"Convert p-value to significance symbols.\"\"\"\n",
    "    if p_value is None or pd.isna(p_value):\n",
    "        return ''\n",
    "    elif p_value < 0.001:\n",
    "        return '***'\n",
    "    elif p_value < 0.01:\n",
    "        return '**'\n",
    "    elif p_value < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Apply statistical tests\n",
    "df['accuracy_pvalue'] = df.apply(lambda row: calculate_accuracy_pvalue(row['tp'], row['tn'], row['fp'], row['fn']), axis=1)\n",
    "df['phi_ci_lower'] = None\n",
    "df['phi_ci_upper'] = None\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    ci_lower, ci_upper = calculate_phi_confidence_interval(row['phi_correlation'], row['samples'])\n",
    "    df.at[idx, 'phi_ci_lower'] = ci_lower\n",
    "    df.at[idx, 'phi_ci_upper'] = ci_upper\n",
    "\n",
    "# Add significance indicators\n",
    "df['accuracy_sig'] = df['accuracy_pvalue'].apply(significance_level)\n",
    "df['phi_significant'] = df['phi_ci_lower'].apply(lambda x: '✓' if x is not None and x > 0 else '')\n",
    "\n",
    "print(\"Statistical tests completed!\")\n",
    "print(f\"Significant accuracy results: {(df['accuracy_sig'] != '').sum()}/{len(df)}\")\n",
    "print(f\"Significant phi correlations: {(df['phi_significant'] == '✓').sum()}/{len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Comprehensive Results Table\n",
    "def format_number(x, decimals=3):\n",
    "    \"\"\"Format number for display.\"\"\"\n",
    "    if pd.isna(x) or x is None:\n",
    "        return 'N/A'\n",
    "    return f'{x:.{decimals}f}'\n",
    "\n",
    "def format_ci(lower, upper, decimals=3):\n",
    "    \"\"\"Format confidence interval for display.\"\"\"\n",
    "    if pd.isna(lower) or pd.isna(upper) or lower is None or upper is None:\n",
    "        return 'N/A'\n",
    "    return f'[{lower:.{decimals}f}, {upper:.{decimals}f}]'\n",
    "\n",
    "# Prepare display table\n",
    "results_table = df.copy()\n",
    "results_table = results_table.sort_values(['mode', 'accuracy'], ascending=[True, False])\n",
    "\n",
    "# Create formatted columns\n",
    "results_table['Accuracy'] = results_table['accuracy'].apply(lambda x: format_number(x, 3)) + ' ' + results_table['accuracy_sig']\n",
    "results_table['Phi (r)'] = results_table['phi_correlation'].apply(lambda x: format_number(x, 3)) + ' ' + results_table['phi_significant']\n",
    "results_table['Accuracy p-value'] = results_table['accuracy_pvalue'].apply(lambda x: format_number(x, 4) if x is not None else 'N/A')\n",
    "results_table['Phi 95% CI'] = results_table.apply(lambda row: format_ci(row['phi_ci_lower'], row['phi_ci_upper']), axis=1)\n",
    "results_table['Confusion Matrix'] = results_table.apply(lambda row: f'TP:{row[\"tp\"]}, TN:{row[\"tn\"]}, FP:{row[\"fp\"]}, FN:{row[\"fn\"]}', axis=1)\n",
    "\n",
    "# Select columns for display\n",
    "display_columns = [\n",
    "    'mode', 'model_name', 'samples', 'Accuracy', 'Accuracy p-value', \n",
    "    'Phi (r)', 'Phi 95% CI', 'Confusion Matrix'\n",
    "]\n",
    "\n",
    "final_table = results_table[display_columns].copy()\n",
    "final_table.columns = ['Mode', 'Model', 'N', 'Accuracy', 'Acc p-value', 'Phi Correlation', 'Phi 95% CI', 'Confusion Matrix']\n",
    "\n",
    "print(\"📊 COMPREHENSIVE STEERING ANALYSIS RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Statistical Significance: *** p<0.001, ** p<0.01, * p<0.05\")\n",
    "print(\"Phi Correlation: ✓ indicates 95% CI excludes zero\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display with nice formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 30)\n",
    "\n",
    "display(final_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model Performance\n",
    "if len(df) > 0:\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Steering Analysis Results: Model Performance Overview', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Accuracy by Mode and Model\n",
    "    ax1 = axes[0, 0]\n",
    "    df_plot = df[df['accuracy'].notna()]\n",
    "    if len(df_plot) > 0:\n",
    "        # Color by significance\n",
    "        colors = ['red' if sig != '' else 'lightblue' for sig in df_plot['accuracy_sig']]\n",
    "        bars = ax1.bar(range(len(df_plot)), df_plot['accuracy'], color=colors, alpha=0.7)\n",
    "        ax1.set_xticks(range(len(df_plot)))\n",
    "        ax1.set_xticklabels([f\"{row['mode']}\\n{row['model_name'][:15]}\" for _, row in df_plot.iterrows()], \n",
    "                           rotation=45, ha='right')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_title('Accuracy by Model (Red = Significant)')\n",
    "        ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Chance level')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Add significance annotations\n",
    "        for i, (_, row) in enumerate(df_plot.iterrows()):\n",
    "            if row['accuracy_sig']:\n",
    "                ax1.annotate(row['accuracy_sig'], (i, row['accuracy'] + 0.01), ha='center', fontweight='bold')\n",
    "    \n",
    "    # 2. Phi Correlation by Mode and Model\n",
    "    ax2 = axes[0, 1]\n",
    "    df_phi = df[df['phi_correlation'].notna()]\n",
    "    if len(df_phi) > 0:\n",
    "        # Color by significance\n",
    "        colors = ['red' if sig == '✓' else 'lightblue' for sig in df_phi['phi_significant']]\n",
    "        bars = ax2.bar(range(len(df_phi)), df_phi['phi_correlation'], color=colors, alpha=0.7)\n",
    "        \n",
    "        # Add error bars for confidence intervals\n",
    "        errors_lower = df_phi['phi_correlation'] - df_phi['phi_ci_lower'].fillna(df_phi['phi_correlation'])\n",
    "        errors_upper = df_phi['phi_ci_upper'].fillna(df_phi['phi_correlation']) - df_phi['phi_correlation']\n",
    "        ax2.errorbar(range(len(df_phi)), df_phi['phi_correlation'], \n",
    "                    yerr=[errors_lower, errors_upper], fmt='none', color='black', capsize=3, alpha=0.7)\n",
    "        \n",
    "        ax2.set_xticks(range(len(df_phi)))\n",
    "        ax2.set_xticklabels([f\"{row['mode']}\\n{row['model_name'][:15]}\" for _, row in df_phi.iterrows()], \n",
    "                           rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Phi Correlation')\n",
    "        ax2.set_title('Phi Correlation with 95% CI (Red = Significant)')\n",
    "        ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 3. Accuracy vs Phi Correlation Scatter\n",
    "    ax3 = axes[1, 0]\n",
    "    df_scatter = df[(df['accuracy'].notna()) & (df['phi_correlation'].notna())]\n",
    "    if len(df_scatter) > 0:\n",
    "        for mode in df_scatter['mode'].unique():\n",
    "            mode_data = df_scatter[df_scatter['mode'] == mode]\n",
    "            ax3.scatter(mode_data['accuracy'], mode_data['phi_correlation'], \n",
    "                       label=mode, alpha=0.7, s=80)\n",
    "        \n",
    "        ax3.set_xlabel('Accuracy')\n",
    "        ax3.set_ylabel('Phi Correlation')\n",
    "        ax3.set_title('Accuracy vs Phi Correlation by Mode')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Sample Sizes\n",
    "    ax4 = axes[1, 1]\n",
    "    if len(df) > 0:\n",
    "        sample_counts = df.groupby(['mode', 'model_name'])['samples'].first().reset_index()\n",
    "        ax4.bar(range(len(sample_counts)), sample_counts['samples'], alpha=0.7, color='lightgreen')\n",
    "        ax4.set_xticks(range(len(sample_counts)))\n",
    "        ax4.set_xticklabels([f\"{row['mode']}\\n{row['model_name'][:15]}\" for _, row in sample_counts.iterrows()], \n",
    "                           rotation=45, ha='right')\n",
    "        ax4.set_ylabel('Sample Size')\n",
    "        ax4.set_title('Sample Sizes by Experiment')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n📈 SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 40)\n",
    "    summary_stats = df.groupby('mode').agg({\n",
    "        'accuracy': ['count', 'mean', 'std', 'min', 'max'],\n",
    "        'phi_correlation': ['mean', 'std', 'min', 'max'],\n",
    "        'samples': ['mean', 'min', 'max']\n",
    "    }).round(3)\n",
    "    \n",
    "    display(summary_stats)\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results\n",
    "if len(df) > 0:\n",
    "    # Save comprehensive results\n",
    "    output_dir = Path('../outputs/feature_classification/analysis')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Export detailed results\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Full results with all columns\n",
    "    full_results_file = output_dir / f'steering_analysis_full_results_{timestamp}.csv'\n",
    "    df.to_csv(full_results_file, index=False)\n",
    "    \n",
    "    # Summary table\n",
    "    summary_file = output_dir / f'steering_analysis_summary_{timestamp}.csv'\n",
    "    final_table.to_csv(summary_file, index=False)\n",
    "    \n",
    "    # JSON summary for programmatic access\n",
    "    json_summary = {\n",
    "        'timestamp': timestamp,\n",
    "        'total_experiments': len(df),\n",
    "        'modes': df['mode'].value_counts().to_dict(),\n",
    "        'models': df['model_name'].nunique(),\n",
    "        'significant_accuracy': (df['accuracy_sig'] != '').sum(),\n",
    "        'significant_phi': (df['phi_significant'] == '✓').sum(),\n",
    "        'best_accuracy': {\n",
    "            'value': df['accuracy'].max(),\n",
    "            'model': df.loc[df['accuracy'].idxmax(), 'model_name'] if not df['accuracy'].isna().all() else None,\n",
    "            'mode': df.loc[df['accuracy'].idxmax(), 'mode'] if not df['accuracy'].isna().all() else None\n",
    "        },\n",
    "        'best_phi': {\n",
    "            'value': df['phi_correlation'].max(),\n",
    "            'model': df.loc[df['phi_correlation'].idxmax(), 'model_name'] if not df['phi_correlation'].isna().all() else None,\n",
    "            'mode': df.loc[df['phi_correlation'].idxmax(), 'mode'] if not df['phi_correlation'].isna().all() else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    json_file = output_dir / f'steering_analysis_summary_{timestamp}.json'\n",
    "    with json_file.open('w') as f:\n",
    "        json.dump(json_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\n💾 RESULTS EXPORTED\")\n",
    "    print(f\"Full results: {full_results_file}\")\n",
    "    print(f\"Summary table: {summary_file}\")\n",
    "    print(f\"JSON summary: {json_file}\")\n",
    "    \n",
    "    # Display final summary\n",
    "    print(f\"\\n🎯 KEY FINDINGS:\")\n",
    "    if json_summary['best_accuracy']['model']:\n",
    "        print(f\"Best Accuracy: {json_summary['best_accuracy']['value']:.3f} ({json_summary['best_accuracy']['model']}, {json_summary['best_accuracy']['mode']})\")\n",
    "    if json_summary['best_phi']['model']:\n",
    "        print(f\"Best Phi Correlation: {json_summary['best_phi']['value']:.3f} ({json_summary['best_phi']['model']}, {json_summary['best_phi']['mode']})\")\n",
    "    print(f\"Statistically Significant Results: {json_summary['significant_accuracy']}/{len(df)} (accuracy), {json_summary['significant_phi']}/{len(df)} (phi)\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n⚠️  No results to export. Run steering analysis experiments first.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
