{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d473acee",
   "metadata": {},
   "source": [
    "# Exploratory Notebook for architecture setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dbc20e",
   "metadata": {},
   "source": [
    "We will be using GPT2 small for quick testing. Currently, I'm following the guide from [here](https://colab.research.google.com/github/jbloomAus/SAELens/blob/main/tutorials/basic_loading_and_analysing.ipynb#scrollTo=sNSfL80Uv611)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf8f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# The above lines are used to automatically reload modules before executing code\n",
    "\n",
    "from fsrl import SAEAdapter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c5131e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPS support maybe?\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "release = \"gpt2-small-res-jb\"\n",
    "sae_id = \"blocks.7.hook_resid_pre\"\n",
    "\n",
    "sae, cfg_dict, sparsity = SAEAdapter.from_pretrained(release, sae_id, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf4cac",
   "metadata": {},
   "source": [
    "We should be mindful of the loading warning. For now, I will ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829279f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAEAdapter(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "  (adapter_activation): ReLU()\n",
      "  (adapter_layers): ModuleList(\n",
      "    (0): Linear(in_features=768, out_features=24576, bias=True)\n",
      "  )\n",
      "  (hook_sae_adapter_pre): HookPoint()\n",
      "  (hook_sae_adapter_post): HookPoint()\n",
      "  (hook_sae_fusion): HookPoint()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(sae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed31268",
   "metadata": {},
   "source": [
    "Hookpoints are used for caching activations in the model. These can be accessed through run_with_cache which stores the activations in a dictionary using the hookpoint names as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d111efa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: standard\n",
      "d_in: 768\n",
      "d_sae: 24576\n",
      "activation_fn_str: relu\n",
      "apply_b_dec_to_input: True\n",
      "finetuning_scaling_factor: False\n",
      "context_size: 128\n",
      "model_name: gpt2-small\n",
      "hook_name: blocks.7.hook_resid_pre\n",
      "hook_layer: 7\n",
      "hook_head_index: None\n",
      "prepend_bos: True\n",
      "dataset_path: Skylion007/openwebtext\n",
      "dataset_trust_remote_code: True\n",
      "normalize_activations: none\n",
      "dtype: torch.float32\n",
      "device: cuda\n",
      "sae_lens_training_version: None\n",
      "activation_fn_kwargs: {}\n",
      "neuronpedia_id: gpt2-small/7-res-jb\n",
      "model_from_pretrained_kwargs: {'center_writing_weights': True}\n",
      "seqpos_slice: (None,)\n",
      "release: gpt2-small-res-jb\n",
      "sae_id: blocks.7.hook_resid_pre\n"
     ]
    }
   ],
   "source": [
    "for k, v in sae.cfg.__dict__.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fad37f",
   "metadata": {},
   "source": [
    "There are some important variables to note:\n",
    "\n",
    "- D_in = Width of the residual stream (e.g. 768 for GPT2 small)\n",
    "- D_sae = Number of features in the SAE (e.g. 24576 for GPT2 small)\n",
    "- hook_name = Name of the hookpoint in transformer_lens on which the SAE was trained on.\n",
    "\n",
    "The neuronpedia_id might also be worth keeping in mind if we want to get labels for the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a635f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Small subset of the Pile\n",
    "dataset = load_dataset(\n",
    "    path=\"NeelNanda/pile-10k\",\n",
    "    split=\"train\",\n",
    "    streaming=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c074ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "# This function turns all dataset examples into tokens, combines then and then splits them into chunks based on the max length\n",
    "# It's useful in a pre-training / unsupervised context, where we have a continuous stream of unlabelled text data \n",
    "# The huggingface tokenizer is unaware of the concatenation step which takes immediately after and thus raises a harmless warning\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,\n",
    "    tokenizer=model.tokenizer,\n",
    "    streaming=False,\n",
    "    max_length=sae.cfg.context_size,\n",
    "    add_bos_token=sae.cfg.prepend_bos,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54fe6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l0_norm(feature_acts: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the average L0 norm for a batch of feature activations.\n",
    "    Ignores the BOS token position.\n",
    "    \"\"\"\n",
    "    # Ensure the tensor is on the CPU and detached for calculation\n",
    "    feature_acts = feature_acts.detach().cpu()\n",
    "    \n",
    "    # Exclude the BOS token [:, 1:] and count non-zero features\n",
    "    l0_per_token = (feature_acts[:, 1:] > 0).float().sum(dim=-1)\n",
    "    \n",
    "    # Return the average L0 norm across all tokens in the batch\n",
    "    return l0_per_token.mean().item()\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05630ca",
   "metadata": {},
   "source": [
    "## Testing SAE Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "637d2571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch tokens shape: torch.Size([32, 128])\n",
      "dict_keys(['hook_sae_input', 'hook_sae_acts_pre', 'hook_sae_acts_post', 'hook_sae_error', 'hook_sae_adapter_pre', 'hook_sae_adapter_post', 'hook_sae_fusion', 'hook_sae_recons', 'hook_sae_output'])\n",
      "Average L0 (Base SAE): 61.42\n",
      "Average L0 (Fused/Steered): 12372.82\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    # Get a batch of tokens from your dataset\n",
    "    batch_tokens = token_dataset[:BATCH_SIZE][\"tokens\"]\n",
    "    print(f\"Batch tokens shape: {batch_tokens.shape}\")\n",
    "\n",
    "    # Get the LLM's activations at the SAE's hook point\n",
    "    _, llm_cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "    sae_input_activations = llm_cache[sae.cfg.hook_name]\n",
    "    \n",
    "    # Use run_with_cache on the SAE itself to get its internal activations\n",
    "    sae_reconstruction, sae_cache = sae.run_with_cache(sae_input_activations)\n",
    "    \n",
    "    # Check the keys in the SAE cache\n",
    "    print(sae_cache.keys())\n",
    "\n",
    "    # Clean up memory\n",
    "    del llm_cache\n",
    "    \n",
    "    # a) L0 norm of the original, unsteered SAE features\n",
    "    base_sae_features = sae_cache[\"hook_sae_acts_post\"]\n",
    "    l0_base = calculate_l0_norm(base_sae_features)\n",
    "    print(f\"Average L0 (Base SAE): {l0_base:.2f}\")\n",
    "\n",
    "    # b) L0 norm of the final, steered/fused features\n",
    "    fused_features = sae_cache[\"hook_sae_fusion\"]\n",
    "    l0_fused = calculate_l0_norm(fused_features)\n",
    "    print(f\"Average L0 (Fused/Steered): {l0_fused:.2f}\")\n",
    "    \n",
    "    # Clean up more memory\n",
    "    del sae_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a89df70",
   "metadata": {},
   "source": [
    "This seems to be working. The l0 norm of the fused vector is very high since the adapter is randomly initialized with small values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95f210",
   "metadata": {},
   "source": [
    "## Testing saving and loading\n",
    "Should probably put these in the test folder at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92b3df46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter saved to ../models/test\n"
     ]
    }
   ],
   "source": [
    "# Testing save\n",
    "save_path = \"../models/test\"\n",
    "sae.save_adapter(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46ccadd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter loaded from ../models/test\n"
     ]
    }
   ],
   "source": [
    "# Testing load\n",
    "sae = SAEAdapter.load_from_pretrained_adapter(save_path, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a382a2",
   "metadata": {},
   "source": [
    "## Testing out the HookedModel for Feature Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c00302e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.8859e-05,  1.6555e-04,  3.9675e-05,  ...,  1.4030e-05,\n",
      "         -7.6774e-05, -4.5187e-05],\n",
      "        [ 1.1209e-04,  3.7414e-06,  1.9793e-04,  ..., -4.4484e-05,\n",
      "         -4.6172e-05, -2.3969e-06],\n",
      "        [-1.7648e-05,  1.6273e-04,  1.9007e-04,  ..., -9.0681e-05,\n",
      "         -1.4844e-04,  1.7187e-04],\n",
      "        ...,\n",
      "        [-1.8724e-04, -3.5663e-05, -1.0638e-04,  ...,  1.9664e-05,\n",
      "          2.3716e-05,  1.1651e-04],\n",
      "        [-1.3992e-05,  2.6648e-05, -4.1684e-05,  ...,  2.8051e-05,\n",
      "          9.1322e-05, -2.3333e-05],\n",
      "        [-5.1574e-05,  1.4032e-04,  8.9403e-05,  ...,  2.0003e-05,\n",
      "         -7.6993e-05, -5.2337e-06]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "from fsrl import HookedModel\n",
    "hooked_model = HookedModel(model, sae)\n",
    "\n",
    "# Will display the SAE's weights and biases\n",
    "print(hooked_model.get_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a80a955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web. Playing on the web works, but you have to simulate multi-touch for table moving and that can be a bit confusing.\n",
      "\n",
      "There’s a lot I’d like to talk about. I’ll go through every topic, insted of making the typical what went right/wrong list.\n",
      "\n",
      "Concept\n",
      "\n",
      "Working over the theme was probably one of the hardest tasks I had to face.\n",
      "\n",
      "Originally, I had an idea of what kind of\n",
      "-----------------------------\n",
      "\n",
      "Decoded output: ,'s a. it it,\n",
      " can submit it�️iv of the Fundes Heroes� on the. iOS you iOS iPhone.\n",
      " on Android web is just too it can to be the-thread gestures it-. table's be a pain of.\n",
      "\n",
      "The are�t a way of want�ve like to say about,\n",
      "'m�d talk over the single I anda, the a table Android� wrong,wrong,,\n",
      "\n",
      "Iclusionss\n",
      "The with the past of a the of the most things I ever to do. I\n",
      "I, I wanted a idea for how I of table\n"
     ]
    }
   ],
   "source": [
    "# Test a forward pass using example tokens\n",
    "example_tokens = batch_tokens[:1]  # Take the first example from the batch\n",
    "\n",
    "# Input\n",
    "print(model.tokenizer.decode(example_tokens[0], skip_special_tokens=True), end=\"\\n-----------------------------\\n\\n\")\n",
    "\n",
    "output = hooked_model(example_tokens)\n",
    "\n",
    "# Decode the output using the tokenizer, take argmax to get the most likely token\n",
    "output = output.logits.argmax(dim=-1)\n",
    "decoded_output = model.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"Decoded output: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9a750b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean decoded output: \n",
      "'s a. it it,\n",
      " can see it�️iv of the Femptes Survivors� on your. iOS you iOS iPhone.\n",
      " on Android web is just too it can to be the-player gestures it-. other's be a pain tricky.\n",
      "\n",
      "The are�t a way of want�ve like to say about.\n",
      "'m�d talk into the single in anda in the a table table� wrong,wrong,,\n",
      "\n",
      "Iclusionss\n",
      "The with the past of a the of the hardest things I had to do. I\n",
      "I, I wanted a idea for how I of game\n"
     ]
    }
   ],
   "source": [
    "clean_output = model(example_tokens)\n",
    "clean_output = clean_output.argmax(dim=-1)\n",
    "clean_decoded_output = model.tokenizer.decode(clean_output[0], skip_special_tokens=True)\n",
    "print(f\"Clean decoded output: {clean_decoded_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d96be",
   "metadata": {},
   "source": [
    "I guess it works? Both outputs should be identical with a newly initialized model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature-steering-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
